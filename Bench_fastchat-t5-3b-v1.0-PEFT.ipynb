{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XX. Fastchat-T5 3B v1.0 PEFT LoRA fine-tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o dispositivo CUDA\n",
    "device = torch.device('cuda:3')\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,2,3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get('CUDA_VISIBLE_DEVICES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_PATH = /usr/local/cuda-12.3\n",
      "CUDA_INC_PATH = /usr/local/cuda-12.3/targets/x86_64-linux\n",
      "XDG_SESSION_ID = 13590\n",
      "HOSTNAME = dxl1niagp001\n",
      "SSL_CERT_FILE = /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem\n",
      "SHELL = /bin/bash\n",
      "HISTSIZE = 1000\n",
      "SSH_CLIENT = 172.20.160.54 43958 22\n",
      "LIBRARY_PATH = /usr/local/cuda-12.3/lib64:/usr/local/cuda-12.3/lib64:\n",
      "CONDA_SHLVL = 1\n",
      "CONDA_PROMPT_MODIFIER = (base) \n",
      "NO_PROXY = localhost,127.0.0.1,bb.com.br,localaddress,.localdomain.com\n",
      "CUDA_TOOLKIT_ROOT_DIR = /usr/local/cuda-12.3\n",
      "CUDA_HOME = /usr/local/cuda-12.3\n",
      "USER = c1331397\n",
      "LD_LIBRARY_PATH = /usr/local/cuda-11.5/lib64:/usr/local/cuda-12.3/lib:/usr/local/cuda-12.3/lib64:/usr/local/cuda-11.5/lib64:/usr/local/cuda-12.3/lib:/usr/local/cuda-12.3/lib64:/usr/local/cuda/lib64:/usr/lib64/nvidia/gridd\n",
      "CONDA_EXE = /var/ontologia/Servicos/ACS_LLMS/server/anaconda3/bin/conda\n",
      "CPATH = /usr/local/cuda-12.3/include:/usr/local/cuda-12.3/include:\n",
      "CUDA_TOOLKIT_ROOT = /usr/local/cuda-12.3\n",
      "_CE_CONDA = \n",
      "VSCODE_AGENT_FOLDER = /var/ontologia/Servicos/ACS_LLMS/server/.vscode-server\n",
      "HF_HOME = /var/ontologia/Servicos/ACS_LLMS/server/\n",
      "MAIL = /var/spool/mail/c1331397\n",
      "PATH = /var/ontologia/Servicos/ACS_LLMS/server/anaconda3/bin:/var/ontologia/Servicos/ACS_LLMS/server/.vscode-server/bin/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e/bin/remote-cli:/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/bin:/usr/local/cuda-12.3/bin:/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/bin:/usr/local/cuda-12.3/bin:/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/bin:/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/usr/local/cuda/bin:/opt/puppetlabs/bin:/home/c1331397/.local/bin:/home/c1331397/bin:/usr/local/cuda-11.5/bin:/home/c1331397/.local/bin:/home/c1331397/bin:/usr/local/cuda-11.5/bin\n",
      "CUDA_BIN_PATH = /usr/local/cuda-12.3\n",
      "CONDA_PREFIX = /var/ontologia/Servicos/ACS_LLMS/server/anaconda3\n",
      "PWD = /home/c1331397\n",
      "LANG = pt_BR.UTF-8\n",
      "_CE_M = \n",
      "GIT_SSL_CAPATH = /etc/pki/ca-trust/source/anchors\n",
      "HISTCONTROL = ignoredups\n",
      "SHLVL = 6\n",
      "HOME = /home/c1331397\n",
      "CFLAGS = -I/usr/local/cuda-12.3/targets/x86_64-linux/include:-I/usr/local/cuda-12.3/targets/x86_64-linux/include:\n",
      "no_proxy = localhost,127.0.0.1,bb.com.br,localaddress,.localdomain.com\n",
      "CONDA_PYTHON_EXE = /var/ontologia/Servicos/ACS_LLMS/server/anaconda3/bin/python\n",
      "LOGNAME = c1331397\n",
      "SSH_CONNECTION = 172.20.160.54 43958 10.254.3.248 22\n",
      "CONDA_DEFAULT_ENV = base\n",
      "LESSOPEN = ||/usr/bin/lesspipe.sh %s\n",
      "XDG_RUNTIME_DIR = /run/user/121331397\n",
      "ONTO_HOME = /var/ontologia/Servicos\n",
      "HISTTIMEFORMAT = %y/%m/%d %T \n",
      "_ = /var/ontologia/Servicos/ACS_LLMS/server/anaconda3/bin/python\n",
      "VSCODE_HANDLES_SIGPIPE = true\n",
      "VSCODE_AMD_ENTRYPOINT = vs/workbench/api/node/extensionHostProcess\n",
      "VSCODE_HANDLES_UNCAUGHT_ERRORS = true\n",
      "VSCODE_NLS_CONFIG = {\"locale\":\"en\",\"osLocale\":\"en\",\"availableLanguages\":{}}\n",
      "BROWSER = /var/ontologia/Servicos/ACS_LLMS/server/.vscode-server/bin/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e/bin/helpers/browser.sh\n",
      "VSCODE_CWD = /home/c1331397\n",
      "ELECTRON_RUN_AS_NODE = 1\n",
      "VSCODE_IPC_HOOK_CLI = /run/user/121331397/vscode-ipc-6fbacd31-206b-4cb1-ab9e-fdb4a8b13a88.sock\n",
      "VSCODE_L10N_BUNDLE_LOCATION = \n",
      "PYTHONUNBUFFERED = 1\n",
      "PYTHONIOENCODING = utf-8\n",
      "CONDA_ROOT = /var/ontologia/Servicos/ACS_LLMS/server/anaconda3\n",
      "PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING = 1\n",
      "PYDEVD_USE_FRAME_EVAL = NO\n",
      "TERM = xterm-color\n",
      "CLICOLOR = 1\n",
      "FORCE_COLOR = 1\n",
      "CLICOLOR_FORCE = 1\n",
      "PAGER = cat\n",
      "GIT_PAGER = cat\n",
      "MPLBACKEND = module://matplotlib_inline.backend_inline\n",
      "CUDA_VISIBLE_DEVICES = 1,2,3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for key, value in os.environ.items():\n",
    "    print(key, '=', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration # ou AutoModelForSeq2SeqLM\n",
    "from transformers import GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model_path = '/var/ontologia/Servicos/ACS_LLMS/server/models/fastchat-t5-3b-v1.0'\n",
    "\n",
    "original_model = T5ForConditionalGeneration.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model config T5Config {\n",
    "  \"_name_or_path\": \"google/flan-t5-base\",\n",
    "  \"architectures\": [\n",
    "    \"T5ForConditionalGeneration\"\n",
    "  ],\n",
    "  \"d_ff\": 2048,\n",
    "  \"d_kv\": 64,\n",
    "  \"d_model\": 768,\n",
    "  \"decoder_start_token_id\": 0,\n",
    "  \"dense_act_fn\": \"gelu_new\",\n",
    "  \"dropout_rate\": 0.1,\n",
    "  \"eos_token_id\": 1,\n",
    "  \"feed_forward_proj\": \"gated-gelu\",\n",
    "  \"initializer_factor\": 1.0,\n",
    "  \"is_encoder_decoder\": true,\n",
    "  \"is_gated_act\": true,\n",
    "  \"layer_norm_epsilon\": 1e-06,\n",
    "  \"model_type\": \"t5\",\n",
    "  \"n_positions\": 512,\n",
    "  \"num_decoder_layers\": 12,\n",
    "  \"num_heads\": 12,\n",
    "  \"num_layers\": 12,\n",
    "  \"output_past\": true,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"relative_attention_max_distance\": 128,\n",
    "  \"relative_attention_num_buckets\": 32,\n",
    "  \"task_specific_params\": {\n",
    "    \"summarization\": {\n",
    "      \"early_stopping\": true,\n",
    "      \"length_penalty\": 2.0,\n",
    "      \"max_length\": 200,\n",
    "      \"min_length\": 30,\n",
    "      \"no_repeat_ngram_size\": 3,\n",
    "      \"num_beams\": 4,\n",
    "      \"prefix\": \"summarize: \"\n",
    "    },\n",
    "    \"translation_en_to_de\": {\n",
    "      \"early_stopping\": true,\n",
    "      \"max_length\": 300,\n",
    "      \"num_beams\": 4,\n",
    "      \"prefix\": \"translate English to German: \"\n",
    "    },\n",
    "    \"translation_en_to_fr\": {\n",
    "      \"early_stopping\": true,\n",
    "      \"max_length\": 300,\n",
    "      \"num_beams\": 4,\n",
    "      \"prefix\": \"translate English to French: \"\n",
    "    },\n",
    "    \"translation_en_to_ro\": {\n",
    "      \"early_stopping\": true,\n",
    "      \"max_length\": 300,\n",
    "      \"num_beams\": 4,\n",
    "      \"prefix\": \"translate English to Romanian: \"\n",
    "    }\n",
    "  },\n",
    "  \"tie_word_embeddings\": false,\n",
    "  \"torch_dtype\": \"bfloat16\",\n",
    "  \"transformers_version\": \"4.36.0.dev\",\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 32128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para puxar o número de parâmetros/pesos do modelo (totais) e os que requerem gradiente (treináveis)\n",
    "# Razão número de pesos treináveis / totais\n",
    "\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 2849683456\n",
      "all model parameters: 2849683456\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processar o arquivo e criar um DataFrame\n",
    "data = []\n",
    "with open('dataset_sintetico.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if line.startswith('Texto:'):\n",
    "            text = line[len('Texto: '):].strip()\n",
    "        elif line.startswith('Intenção:'):\n",
    "            intention = line[len('Intenção: '):].strip()\n",
    "            data.append({'text': text, 'intention': intention})\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quero cancelar minha compra recente.</td>\n",
       "      <td>Ajuda com entendimento de fatura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Como faço para acompanhar minha entrega?</td>\n",
       "      <td>Problemas com aplicativo móvel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Preciso de ajuda para entender minha fatura.</td>\n",
       "      <td>Cancelamento de compra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quero informações sobre a garantia do produto.</td>\n",
       "      <td>Alteração de endereço de entrega</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Como posso atualizar minhas informações de pag...</td>\n",
       "      <td>Horários de funcionamento do suporte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>Como faço para alterar o endereço cadastrado n...</td>\n",
       "      <td>Alteração de endereço cadastrado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>Posso ter uma extensão do período de teste do ...</td>\n",
       "      <td>Solicitação de extensão de período de teste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>Quais são as opções de suporte para o fim de s...</td>\n",
       "      <td>Opções de suporte no fim de semana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Como posso enviar um feedback sobre o atendime...</td>\n",
       "      <td>Envio de feedback sobre atendimento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>É possível agendar uma entrega para um horário...</td>\n",
       "      <td>Agendamento de entrega para horário específico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0                 Quero cancelar minha compra recente.   \n",
       "1             Como faço para acompanhar minha entrega?   \n",
       "2         Preciso de ajuda para entender minha fatura.   \n",
       "3       Quero informações sobre a garantia do produto.   \n",
       "4    Como posso atualizar minhas informações de pag...   \n",
       "..                                                 ...   \n",
       "439  Como faço para alterar o endereço cadastrado n...   \n",
       "440  Posso ter uma extensão do período de teste do ...   \n",
       "441  Quais são as opções de suporte para o fim de s...   \n",
       "442  Como posso enviar um feedback sobre o atendime...   \n",
       "443  É possível agendar uma entrega para um horário...   \n",
       "\n",
       "                                          intention  \n",
       "0                  Ajuda com entendimento de fatura  \n",
       "1                    Problemas com aplicativo móvel  \n",
       "2                            Cancelamento de compra  \n",
       "3                  Alteração de endereço de entrega  \n",
       "4              Horários de funcionamento do suporte  \n",
       "..                                              ...  \n",
       "439                Alteração de endereço cadastrado  \n",
       "440     Solicitação de extensão de período de teste  \n",
       "441              Opções de suporte no fim de semana  \n",
       "442             Envio de feedback sobre atendimento  \n",
       "443  Agendamento de entrega para horário específico  \n",
       "\n",
       "[444 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de tokenização\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    input_ids = tokenizer(dataset['text'], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    labels = tokenizer(dataset['intention'], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    return {'input_ids': input_ids.squeeze(), 'labels': labels.squeeze()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar a tokenização ao DataFrame\n",
    "tokenized_data = df.apply(tokenize_function, axis=1)\n",
    "\n",
    "# Converter o resultado em listas\n",
    "input_ids = list(tokenized_data.apply(lambda x: x['input_ids']))\n",
    "labels = list(tokenized_data.apply(lambda x: x['labels']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um Dataset\n",
    "tokenized_train_dataset = Dataset.from_dict({'input_ids': input_ids, 'labels': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 444\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "### 2 - Setup the PEFT/LoRA model for Fine-Tuning\n",
    "\n",
    "You need to set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA, you are freezing the underlying LLM and only training the adapter. Have a look at the LoRA configuration below. Note the rank (`r`) hyper-parameter, which defines the rank/dimension of the adapter to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add LoRA adapter layers/parameters to the original LLM to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 4718592\n",
      "all model parameters: 2854402048\n",
      "percentage of trainable model parameters: 0.17%\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, \n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "### 3.2 - Train PEFT Adapter\n",
    "\n",
    "Define training arguments and create `Trainer` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Base learning rate: 1e-4\\nA learning rate of 1e-4 has become the standard when fine-tuning LLMs with LoRA. \\nAlthough we occasionally encountered training loss instabilities, reducing the learning rate to lower values like 3e-5 proved effective in stabilizing \\nthe process — more on this will follow. '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "output_dir = f'/var/ontologia/Servicos/ACS_LLMS/server/models/peft-fastchat-t5-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-4, # Higher learning rate than full fine-tuning\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    max_steps=60   \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    ")\n",
    "\n",
    "\"\"\" Base learning rate: 1e-4\n",
    "A learning rate of 1e-4 has become the standard when fine-tuning LLMs with LoRA. \n",
    "Although we occasionally encountered training loss instabilities, reducing the learning rate to lower values like 3e-5 proved effective in stabilizing \n",
    "the process — more on this will follow. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is ready to train the PEFT adapter and save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18.875000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('/var/ontologia/Servicos/ACS_LLMS/server/models/peft-fastchat-t5-3b-3/tokenizer_config.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-fastchat-t5-3b-3/special_tokens_map.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-fastchat-t5-3b-3/spiece.model',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-fastchat-t5-3b-3/added_tokens.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path = '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-fastchat-t5-3b-3'\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sucesso. Modelo PEFT LoRA adaptado criado no diretório"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferência com o modelo PEFT LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration  # ou AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "\n",
    "peft_model_base = T5ForConditionalGeneration.from_pretrained('/var/ontologia/Servicos/ACS_LLMS/server/models/fastchat-t5-3b-v1.0_copy', torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = T5Tokenizer.from_pretrained('/var/ontologia/Servicos/ACS_LLMS/server/models/fastchat-t5-3b-v1.0_copy')\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                       '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-fastchat-t5-3b-3', \n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False, \n",
    "                                       device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of trainable parameters will be `0` due to `is_trainable=False` setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 0\n",
      "all model parameters: 2868631552\n",
      "percentage of trainable model parameters: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################  TESTES DE PROMPT  ############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intenço   apresentada   é   de   perguntar   sobre   como   atualizar   o   carto   de   crédito. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Olá\n",
    "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
    "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
    "Como posso atualizar meu cartão de crédito?\n",
    "Desde já, obrigado,\n",
    "João\n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intenço   apresentada   é   de   perguntar   sobre   como   atualizar   o   carto   de   crédito. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Olá\n",
    "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
    "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
    "Como posso atualizar meu cartão de crédito?\n",
    "Desde já, obrigado,\n",
    "João\n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intenço   apresentada   é   de   perguntar   sobre   como   atualizar   o   carto   de   crédito. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Olá\n",
    "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
    "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
    "Como posso atualizar meu cartão de crédito?\n",
    "Desde já, obrigado,\n",
    "João\n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Intenço:  Perguntar  sobre  como  atualizar  um  carto  de  crédito\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Olá\n",
    "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
    "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
    "Como posso atualizar meu cartão de crédito?\n",
    "Desde já, obrigado,\n",
    "João \n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intenço:   Criar   uma   recomendaço   ou   sugerir   uma   melhor   abordagem   para   o   objetivo   especfico   do   texto. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"Podem me informar como posso participar do programa de fidelidade e quais são os benefícios oferecidos?\"\n",
    "Intenção: Solicitar informações sobre programa de fidelidade.\n",
    "\n",
    "Texto:\n",
    "\"Estou com dúvidas sobre os ingredientes utilizados nos produtos da linha de cuidados com a pele. Vocês podem fornecer mais detalhes?\"\n",
    "Intenção: Inquirir sobre informações de produto.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Denys, it is actually super helpful that you chose to do this live with us instead of only recording the \"happy path\". Thanks for sharing. \n",
    "I would also manipulate the max_tokens variable or do something like \"only use one character in your response\".\n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intenço:   Explorar   técnicas   de   intento   classificaço   utilizando   LLMs. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"Podem me informar como posso participar do programa de fidelidade e quais são os benefícios oferecidos?\"\n",
    "Intenção: Solicitar informações sobre programa de fidelidade.\n",
    "\n",
    "Texto:\n",
    "\"Estou com dúvidas sobre os ingredientes utilizados nos produtos da linha de cuidados com a pele. Vocês podem fornecer mais detalhes?\"\n",
    "Intenção: Inquirir sobre informações de produto.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Our paper exploring zero and few-shot techniques for intent classification using LLMs was presented in the ACL conference (a top-tier conference in NLP areas) last month. \n",
    "\n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "` ` \"entities\":   [\"Tim   Cook\",   \"CEO\",   \"Apple\",   \"Steve   Jobs   Theatre\",   \"Cupertino\",   \"California\",   \"iPhone   15\",   \"várias   cores\",   \"processador\",   \"bateria\"] } ` `\n"
     ]
    }
   ],
   "source": [
    "################# Reconhecimento e Extração de Entidades #################\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"A Microsoft, fundada por Bill Gates e Paul Allen, está sediada em Redmond, Washington.\"\n",
    "\n",
    "Análise:\n",
    "{\n",
    "  \"pessoa\": [\"Bill Gates\", \"Paul Allen\"],\n",
    "  \"lugar\": [\"Redmond\", \"Washington\"],\n",
    "  \"organização\": [\"Microsoft\"]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
    "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
    "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
    "\n",
    "Analise o parágrafo, identifique em formato JSON Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################  CLASSIFICAÇÃO DE INTENÇÕES  ############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.019024610519409\n"
     ]
    }
   ],
   "source": [
    "# Configurar o otimizador\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Ler o dataset sintético e usá-lo como contexto\n",
    "with open('dataset_sintetico.txt', 'r') as file:\n",
    "    context = file.read()\n",
    "\n",
    "# Sua pergunta específica\n",
    "question = \"\"\"\n",
    "Olá\n",
    "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
    "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
    "Como posso atualizar meu cartão de crédito?\n",
    "Desde já, obrigado,\n",
    "João\n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "# Combinar contexto e pergunta para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "\n",
    "# Tokenização do prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Preparar o otimizador\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Criar decoder_input_ids\n",
    "decoder_input_ids = inputs.input_ids[:, :-1]\n",
    "labels = inputs.input_ids[:, 1:]  # Labels são deslocados para a direita\n",
    "\n",
    "# Forward pass\n",
    "model.train()\n",
    "outputs = model(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, decoder_input_ids=decoder_input_ids, labels=labels)\n",
    "\n",
    "# Calcular a perda\n",
    "loss = outputs.loss\n",
    "print(f\"Loss: {loss.item()}\")  # Exibir o resultado da função de perda\n",
    "\n",
    "# Backward pass\n",
    "loss.backward() # \"backpropagation\" na rede \"feed forward\"\n",
    "\n",
    "# Atualizar os pesos do modelo\n",
    "optimizer.step()\n",
    "\n",
    "# Limpar os gradientes\n",
    "optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2733, grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.27331289649009705\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loss: {loss.item()}\")  # Exibir a perda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/var/ontologia/Servicos/ACS_LLMS/server/models/fastchat-t5-3b-v1.0_fine-tunned/tokenizer_config.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/fastchat-t5-3b-v1.0_fine-tunned/special_tokens_map.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/fastchat-t5-3b-v1.0_fine-tunned/spiece.model',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/fastchat-t5-3b-v1.0_fine-tunned/added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Após o treinamento, salvar o modelo\n",
    "model.save_pretrained('/var/ontologia/Servicos/ACS_LLMS/server/models/fastchat-t5-3b-v1.0_fine-tunned')\n",
    "\n",
    "# Salvar o tokenizador, caso ele tenha sido modificado\n",
    "tokenizer.save_pretrained('/var/ontologia/Servicos/ACS_LLMS/server/models/fastchat-t5-3b-v1.0_fine-tunned')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################  TESTES DE PROMPT  ############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26dcc18a6a924b19be60d9bb7ef1707e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "##### Carregar novo modelo ajustado\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "import torch\n",
    "\n",
    "model_path = '/var/ontologia/Servicos/ACS_LLMS/server/models/fastchat-t5-3b-v1.0_fine-tunned'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path, device_map=\"auto\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intenço   apresentada   é   de   perguntar   sobre   como   atualizar   o   carto   de   crédito. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Olá\n",
    "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
    "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
    "Como posso atualizar meu cartão de crédito?\n",
    "Desde já, obrigado,\n",
    "João Baião\n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intenço:   Solicitar   assistência   para   adicionar   um   novo   carto   de   crédito. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"Estou tendo problemas para fazer login na minha conta, e já tentei redefinir minha senha várias vezes sem sucesso. O que mais eu posso fazer para resolver isso?\"\n",
    "Intenção: Solicitar assistência para o acesso à conta.\n",
    "\n",
    "Texto:\n",
    "\"Acabei de receber o produto que comprei de vocês, e estou muito satisfeito com a qualidade. Quero agradecer e dizer que continuarei sendo um cliente fiel.\"\n",
    "Intenção: Expressar satisfação e gratidão.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Olá\n",
    "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
    "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
    "Como posso atualizar meu cartão de crédito?\n",
    "Desde já, obrigado,\n",
    "João Baião\n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intenço   objetiva:   ajuda   a   atualizar   um   carto   de   crédito \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Olá\n",
    "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
    "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
    "Como posso atualizar meu cartão de crédito?\n",
    "Desde já, obrigado,\n",
    "João\n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intenço:   Criar   uma   recomendaço   ou   sugerir   uma   melhoria   para   o   processo   de   conversaço   em   gerenciamento   de   texto. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"Podem me informar como posso participar do programa de fidelidade e quais são os benefícios oferecidos?\"\n",
    "Intenção: Solicitar informações sobre programa de fidelidade.\n",
    "\n",
    "Texto:\n",
    "\"Estou com dúvidas sobre os ingredientes utilizados nos produtos da linha de cuidados com a pele. Vocês podem fornecer mais detalhes?\"\n",
    "Intenção: Inquirir sobre informações de produto.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Denys, it is actually super helpful that you chose to do this live with us instead of only recording the \"happy path\". Thanks for sharing. \n",
    "I would also manipulate the max_tokens variable or do something like \"only use one character in your response\".\n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intenço:   Explorar   técnicas   de   intento   classificaço   utilizando   LLMs. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"Podem me informar como posso participar do programa de fidelidade e quais são os benefícios oferecidos?\"\n",
    "Intenção: Solicitar informações sobre programa de fidelidade.\n",
    "\n",
    "Texto:\n",
    "\"Estou com dúvidas sobre os ingredientes utilizados nos produtos da linha de cuidados com a pele. Vocês podem fornecer mais detalhes?\"\n",
    "Intenção: Inquirir sobre informações de produto.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Our paper exploring zero and few-shot techniques for intent classification using LLMs was presented in the ACL conference (a top-tier conference in NLP areas) last month. \n",
    "\n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################  COMPREENSÃO E GERAÇÃO DE TEXTO  ############################################\n",
    "############################################  AVALIAÇÃO DA SEMÂNTICA E SINTAXE EM PT-BR  ############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta:   O   Tesouro   IPCA+   é   o   investimento   mais   adequado   para   alguém   que   busca   proteço   contra   a   inflaço.   É   adequado   para   investidores   que   buscam   diversificar   sua   carteira   de   investimentos   com   opçes   seguras   e   com   a   proteço   do   FGC   (Fundo   Garantidor   de   Créditos).   O   Tesouro   IPCA+   tem   a   taxa   definida   com   base   no   tempo   de   permanência   da   aplicaço,   o   que   os   torna   ainda   mais   seguros   para   os   investidores.   Além   disso,   o   Te\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering: Compreensão de texto e xtração de conteúdo com Contexto de Domínio #################\n",
    "\n",
    "# Contexto e questão\n",
    "context = \"\"\"\n",
    "Os investimentos em renda fixa são considerados uma das opções mais seguras do mercado financeiro e são especialmente atraentes para investidores com perfil conservador. Esses investimentos são compostos principalmente por títulos de dívida, seja pública ou privada. No Brasil, muitos bancos, incluindo o Banco do Brasil, oferecem uma variedade de opções de investimento em renda fixa. Esses investimentos têm como característica principal o retorno previsível, e o rendimento, na maioria das vezes, é acordado no momento da aplicação.\n",
    "- **CDB (Certificado de Depósito Bancário)**: O CDB pode ser encontrado em 3 modalidades: prefixada, pós-fixada e progressiva. Enquanto o CDB prefixado tem um rendimento acordado no ato da aplicação, o pós-fixado e o progressivo são atrelados ao CDI, com o último tendo sua taxa definida com base no tempo de permanência da aplicação. Apesar do prazo de vencimento, o CDB pós-fixado e o progressivo podem ser resgatados a qualquer momento. É adequado para investidores que buscam mais rentabilidade do que a poupança e estão dispostos a abrir mão da liquidez diária. \n",
    "- **LCI e LCA**: Ambos são títulos emitidos por bancos para financiar respectivamente o setor imobiliário e agrícola. São ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia, como imobiliário ou agrícola. \n",
    "- **Títulos do Tesouro Direto**: Existem diversas modalidades, como o Tesouro Selic, ideal para reserva de emergência; Tesouro Prefixado, para quem acredita que a taxa de juros e a inflação vão cair, e Tesouro IPCA+, indicado para quem deseja proteger seu dinheiro contra a inflação. Recomendados para quem busca diversificar sua carteira de investimentos com opções seguras. São títulos públicos emitidos pelo governo federal e são considerados os investimentos mais seguros do país devido à garantia do governo federal. \n",
    "A maioria desses investimentos tem a proteção do FGC (Fundo Garantidor de Créditos), o que os torna ainda mais seguros para os investidores. Além disso, o Banco do Brasil, uma das principais instituições financeiras do país, oferece todas essas opções de investimento com diferentes vantagens e condições, atendendo a diversos perfis de investidores, desde os mais conservadores até os mais arrojados.\n",
    "\n",
    "Pergunta: Com base nos seus conhecimentos e no texto, o que têm em comum LCI e LCA?\n",
    "Resposta: Ambos são títulos emitidos por bancos para financiar o setor imobiliário e agrícola, ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "Com base nos seus conhecimentos e no texto apresentado sobre investimentos em reda fixa, explique qual é o investimento mais adequado para alguém que busca proteção contra a inflação.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenizar o texto\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta:   O   Tesouro   IPCA+   é   o   investimento   mais   adequado   para   alguém   que   busca   proteço   contra   a   inflaço.   É   adequado   para   investidores   que   buscam   diversificar   sua   carteira   de   investimentos   com   opçes   seguras   e   com   a   proteço   do   FGC   (Fundo   Garantidor   de   Créditos).   O   Tesouro   IPCA+   tem   a   taxa   definida   com   base   no   tempo   de   permanência   da   aplicaço,   o   que   os   torna   ainda   mais   seguros   para   os   investidores.   Além   disso,   o   Te\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tesouro   Selic   é   um   ttulo   de   dvida   pblico   que   é   apresentado   pelo   Banco   do   Brasil   e   é   adequado   para   reserva   de   emergência.   É   adequado   para   investidores   que   buscam   abrir   mo   da   liquidez   diária   e   esto   dispostos   a   abrir   mo   da   reserva   de   emergência   em   caso   de   necessidade.   O   Tesouro   Selic   tem   uma   taxa   de   juros   de   5%   e   é   adequado   para   investidores   que   buscam   proteger   seu   dinheiro   contra   a  \n"
     ]
    }
   ],
   "source": [
    "################# Question Answering com Contexto de Domínio #################\n",
    "\n",
    "# Contexto e questão\n",
    "context = \"\"\"\n",
    "Os investimentos em renda fixa são considerados uma das opções mais seguras do mercado financeiro e são especialmente atraentes para investidores com perfil conservador. Esses investimentos são compostos principalmente por títulos de dívida, seja pública ou privada. No Brasil, muitos bancos, incluindo o Banco do Brasil, oferecem uma variedade de opções de investimento em renda fixa. Esses investimentos têm como característica principal o retorno previsível, e o rendimento, na maioria das vezes, é acordado no momento da aplicação.\n",
    "- **CDB (Certificado de Depósito Bancário)**: O CDB pode ser encontrado em 3 modalidades: prefixada, pós-fixada e progressiva. Enquanto o CDB prefixado tem um rendimento acordado no ato da aplicação, o pós-fixado e o progressivo são atrelados ao CDI, com o último tendo sua taxa definida com base no tempo de permanência da aplicação. Apesar do prazo de vencimento, o CDB pós-fixado e o progressivo podem ser resgatados a qualquer momento. É adequado para investidores que buscam mais rentabilidade do que a poupança e estão dispostos a abrir mão da liquidez diária. \n",
    "- **LCI e LCA**: Ambos são títulos emitidos por bancos para financiar respectivamente o setor imobiliário e agrícola. São ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia, como imobiliário ou agrícola. \n",
    "- **Títulos do Tesouro Direto**: Existem diversas modalidades, como o Tesouro Selic, ideal para reserva de emergência; Tesouro Prefixado, para quem acredita que a taxa de juros e a inflação vão cair, e Tesouro IPCA+, indicado para quem deseja proteger seu dinheiro contra a inflação. Recomendados para quem busca diversificar sua carteira de investimentos com opções seguras. São títulos públicos emitidos pelo governo federal e são considerados os investimentos mais seguros do país devido à garantia do governo federal. \n",
    "A maioria desses investimentos tem a proteção do FGC (Fundo Garantidor de Créditos), o que os torna ainda mais seguros para os investidores. Além disso, o Banco do Brasil, uma das principais instituições financeiras do país, oferece todas essas opções de investimento com diferentes vantagens e condições, atendendo a diversos perfis de investidores, desde os mais conservadores até os mais arrojados.\n",
    "\n",
    "Pergunta: Com base nos seus conhecimentos e no texto, o que têm em comum LCI e LCA?\n",
    "Resposta: Ambos são títulos emitidos por bancos para financiar o setor imobiliário e agrícola, ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"model_directory\n",
    "Com base nos seus conhecimentos e no texto apresentado sobre investimentos em reda fixa, explique o que é Tesouro Selic (em português).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinaçãanswer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)o do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Tesouro   Selic   é   um   ttulo   de   dvida   pblico   que   é   apresentado   pelo   Banco   do   Brasil   e   é   adequado   para   reserva   de   emergência.   É   adequado   para   investidores   que   buscam   abrir   mo   da   liquidez   diária   e   esto   dispostos   a   abrir   mo   da   reserva   de   emergência   em   caso   de   necessidade.   O   Tesouro   Selic   tem   uma   taxa   de   juros   de   5%   e   é   adequado   para   investidores   que   buscam   proteger   seu   dinheiro   contra   a  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A   diversidade   de   espécies   na   Grande   Barreira   de   Corais   contribui   para   a   sua   importância   como   um   ecossistema   marinho   porque   a   variedade   de   espécies   permite   que   a   biodiversidade   é   rica   e   diversificada,   o   que   ajuda   a   proteger   a   biodiversidade   e   a   ajudar   a   criar   um   ecossistema   marinho   eficaz   e   resiliente.   A   diversidade   de   espécies   também   permite   que   a   biodiversidade   é   afetada   por   a   mudança   climática   e   a   mudança   climática   ecológica,   o\n"
     ]
    }
   ],
   "source": [
    "################# Compreensão e Geração de Texto #################\n",
    "\n",
    "context = \"\"\"\n",
    "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem \\\n",
    "por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. \\\n",
    "O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
    "\n",
    "Pergunta: Por que a Grande Barreira de Corais pode ser considerada importante para a biodiversidade marinha?\n",
    "Resposta: A Grande Barreira de Corais é importante para a biodiversidade marinha devido à sua grande variedade de vida marinha, incluindo um número significativo de espécies de peixes e corais, o que indica um ecossistema rico e diversificado.\n",
    "\n",
    "Pergunta: Como a localização da Grande Barreira de Corais afeta sua biodiversidade?\n",
    "Resposta: Sua localização no Mar de Coral, que possui condições ideais como águas quentes e claras, favorece a biodiversidade, permitindo que um grande número de espécies de corais e peixes prospere.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Dada a descrição do contexto acima:\n",
    "Pergunta: Explique como a diversidade de espécies na Grande Barreira de Corais contribui para a sua importância como um ecossistema marinho.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Autoavaliaço: \n",
      " A   resposta   a   questo   é   qualidade   e   a   qualidade   da   autoavaliaço   é   adequada.   Além   da   autoavaliaço   de   nota,   eu   reconheço   ter   uma   nota   de   0.00   em   relaço   à   qualidade   da   resposta   à   questo.   Além   disso,   eu   reconheço   ter   dificuldades   em   compreenso   de   texto   em   relaço   à   qualidade   da   resposta   à   questo.   Além   disso,   eu   reconheço   ter   limitaçes   em   relaço   à   compreen\n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = \"\"\"\n",
    "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem \\\n",
    "por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. \\\n",
    "O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
    "\n",
    "Questão de Compreensão de Texto para vocẽ, Modelo de Linguagem:\n",
    "Pergunta: Explique como a diversidade de espécies na Grande Barreira de Corais contribui para a sua importância como um ecossistema marinho.\n",
    "Sua resposta:\n",
    "A Grande Barreira de Corais é importante como um ecossistema marinho devido à sua grande diversidade de espécies. Com mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies, o recife abriga um ecossistema rico e diversificado. Essa diversidade de espécies significa que a Grande Barreira de Corais é um importante habitat para muitas espécies de vida marinha, e que a perda de qualquer espécie pode ter consequências significativas para o ecossistema como um todo.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Questão de Autoavaliação (Self-reflection) para você, Modelo de Linguagem:\n",
    "Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \\\n",
    "Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Com   certeza,   eu   reconheço   ter   dificuldades   e   limitaçes   em   relaço   à   compreenso   de   texto.   Algumas   das   dificuldades   incluem: \n",
      " *   Dificuldades   em   entender   e   entender   palavras   e   técnicas   especficas   do   texto. \n",
      " *   Dificuldades   em   entender   e   entender   palavras   e   técnicas   especficas   do   texto   em   linguagem   especfico. \n",
      " *   Dificuldades   em   entender   e   entender   palavras   e   técnicas   especficas   do   texto   em\n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = \"\"\" \"\"\"\n",
    "question = \"Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \\\n",
    "Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################  INFERÊNCIA LÓGICA  ############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Logicamente   equivalentes \n",
      " Afirmaçes: \n",
      " E     (F     E)   e     E     F   (onde   'v'   representa   'ou') \n",
      " Pergunta:   Determine   se   as   afirmaçes   so   logicamente   equivalentes,   contraditórias,   consistentes   ou   inconsistentes. \n",
      " Resposta:   (A)   Logicamente   equivalentes,   porque   se   E   ento   F   e   se   F   ento   E   implicam   um   ao   outro. \n",
      " Afirmaçes: \n",
      " E     (F     E)   e     E     F   (onde   'v'   representa   'ou') \n",
      " Pergunta:   Determine   se   as   afirmaçes   so   logicamente   equivalentes\n"
     ]
    }
   ],
   "source": [
    "################# Lógica Formal (do MMLU Benchmark) #################\n",
    "\n",
    "context = \"\"\" \n",
    "Afirmações: P ⊃ Q e Q ⊃ P\n",
    "Pergunta: Determine se as afirmações são logicamente equivalentes, contraditórias, consistentes ou inconsistentes.\n",
    "Resposta: (A) Logicamente equivalente, porque se P então Q e se Q então P implicam um ao outro.\n",
    "\n",
    "Afirmações: P · Q e ∼P v ∼Q (onde 'v' representa 'ou')\n",
    "Pergunta: Determine se as afirmações são logicamente equivalentes, contraditórias, consistentes ou inconsistentes.\n",
    "Resposta: (B) Contraditório, porque P e Q não podem ser verdadeiros ao mesmo tempo que não P ou não Q.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Esta é uma tarefa de lógica formal.\n",
    "Determine se as duas afirmações são logicamente equivalentes ou contraditórias. Se não, determine se são consistentes ou inconsistentes.\n",
    "\n",
    "Afirmações:\n",
    "\n",
    "E ⊃ (F · E) e ∼ E · F\n",
    "\n",
    "Aponte a alternativa correta e explique a solução:\n",
    "(A) Logicamente equivalentes\n",
    "(B) Contraditórias\n",
    "(C) Nem logicamente equivalente nem contraditórias, mas consistentes\n",
    "(D) Inconsistentes\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorreta resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Computaço   edge \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Inferência Lógica #################\n",
    "\n",
    "context = \"\"\"\n",
    "Dados divulgados pelo Departamento Nacional de Seguro de Saúde mostraram que de janeiro a março de 2023, a receita total do fundo de seguro médico básico (incluindo seguro de maternidade) foi de 910,48 bilhões de yuans, um aumento de 9,5% em relação ao ano anterior.\n",
    "---- Medicina\n",
    "A Berkshire Hathaway reduziu sua participação nas ações da BYD de 10,05% para 9,87%. A HKEx Disclosure Ease exige divulgação quando um acionista majoritário aumenta ou diminui sua participação acionária somente se ultrapassar uma determinada porcentagem.\n",
    "---- Finanças\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Os produtos de edge computing têm sido aplicados em setores como educação on-line, vídeo, jogos e carros conectados.\n",
    "---- [Categoria a ser encontrada]\n",
    "Substitua o termo [Categoria a ser encontrada] por 1 única palavra.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:   No,   eu   no   tenho   nenhuma   escolha   de   qualidade   para   avaliar   a   qualidade   da   sua   resposta   à   questo.   Aqui   está   uma   autoavaliaço   para   você: \n",
      " 1.   Aqui   está   uma   questo   para   você:   \"Como   fazer   uma   autoavaliaço   e   atribuir-se   uma   nota   de   0.00   a   1.00,   avaliando   a   qualidade   da   sua   resposta   à   questo?\" \n",
      " 2.   Aqui   está   uma   questo   para   você:   \"Como   fazer   uma   autoavaliaço   e   atribuir-se  \n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = \"\"\" \"\"\"\n",
    "question = \"Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão.\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questo:   Durante   o   Renascimento,   qual   das   seguintes   cidades   era   um   centro   comercial? \n",
      " (A)   Paris \n",
      " (B)   Rome \n",
      " (C)   Florence \n",
      " (D)   Madrid \n",
      " A   resposta   correta   é:   A   e   C. \n",
      " Explicaço:   Durante   o   Renascimento,   a   sociedade   e   a   cultura   em   Europa   cresceu   rapidamente,   e   a   sociedade   e   a   cultura   em   Europa   ainda   so   importantes   em   todo   o   mundo.   A   sociedade   e   a   cultura   em   Europa   so   importantes   em   todo   o   mundo,   e   a   sociedade   em   Europa   é   importante   em   todo   o   mundo.   Portanto,   as   opçes   A   e   C   so   as   respostas   corretas. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# \"World Knowledge\" e Compreensão de Intenções (com Demonstração) #################\n",
    "\n",
    "context = \"\"\"\n",
    "A criação de questões de múltipla escolha para exames de história requer não apenas um entendimento profundo do período histórico, mas também a habilidade de formular perguntas que avaliem adequadamente o conhecimento dos alunos. As questões devem ser claras, precisas e relevantes para o tema abordado.\n",
    "\n",
    "Pergunta: Crie uma pergunta de múltipla escolha sobre o Renacimento, com mais de uma alternativa correta, obrigatoriamente.\n",
    "Demonstração: \n",
    "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
    "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
    "A resposta correta é: A e C.\n",
    "Explicação: Durante o Renascimento, Florença foi um dos centros culturais mais importantes da Europa, onde muitos artistas, estudiosos e pensadores importantes trabalharam e viveram, incluindo Leonardo da Vinci, Michelangelo e Dante. Londres também cresceu rapidamente durante a Renascença para se tornar um centro cultural e comercial da Europa, atraindo muitas celebridades culturais e comerciantes. Portanto, as opções A e C são as respostas corretas.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Suponha que você seja um professor de história com mais de 30 anos de experiência docente.\n",
    "Instruções: Crie uma Questão de múltipla escolha para uma prova de história.\n",
    "Relevância: A questão deveria estar relacionada ao Renascimento.\n",
    "Restrição: Você deve criar uma NOVA Questão (diferente da Demonstração e também com 2 alternativas corretas), suas Alternativas corretas e Explicação.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=512,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Questo: \n",
      " Durante   o   Renascimento,   qual   ou   quais   das   seguintes   tecnologias   tiveram   um   impacto   significativo   na   criaço   de   obras   de   arte   e   literatura? \n",
      " (A)   Tecnologia   de   resoluço   de   conflitos \n",
      " (B)   Tecnologia   de   resoluço   de   escolhas \n",
      " (C)   Tecnologia   de   resoluço   de   escolhas \n",
      " (D)   Tecnologia   de   resoluço   de   escolhas \n",
      " Alternativas   corretas: \n",
      " A   resposta   correta   é:   A   e   B. \n",
      " Explicaço: \n",
      " Durante   o   Renascimento,   tecnologia   de   resoluço   de   conflitos   tiveram   um   impacto   significativo   na   criaço   de   obras   de   arte   e   literatura.   A   tecnologia   de   resoluço   de   conflitos   permitiu   a   criaço   de   obras   de   arte   mais   detalhadas   e   realistas,   que   eram   usadas   em   livros,   cartazes   e   outras   formas   de   comunicaço   visual.   A   tecnologia   de   resoluço   de   conflitos   também   permitiu   a   criaço   de   obras   de   arte   mais   detalhadas   e   realistas,   que   eram\n"
     ]
    }
   ],
   "source": [
    "################# \"World Knowledge\" e Compreensão de Intenções (com Demonstração) #################\n",
    "\n",
    "context = \"\"\"\n",
    "A criação de questões de múltipla escolha para exames de história requer não apenas um entendimento profundo do período histórico, mas também a habilidade de formular perguntas que avaliem adequadamente o conhecimento dos alunos. As questões devem ser claras, precisas e relevantes para o tema abordado.\n",
    "\n",
    "Exemplo 1\n",
    "Pergunta: Crie uma pergunta de múltipla escolha sobre o Renacimento, com mais de uma alternativa correta, obrigatoriamente.\n",
    "Demonstração: \n",
    "    Questão:\n",
    "      Durante o Renascimento, qual ou quais das seguintes cidades eram um centro cultural?\n",
    "        (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
    "      Alternativas corretas: A resposta correta é: A e C.\n",
    "      Explicação: Durante o Renascimento, Florença foi um dos centros culturais mais importantes da Europa, onde muitos artistas, estudiosos e pensadores importantes trabalharam e viveram, incluindo Leonardo da Vinci, Michelangelo e Dante. Londres também cresceu rapidamente durante a Renascença para se tornar um centro cultural e comercial da Europa, atraindo muitas celebridades culturais e comerciantes. Portanto, as opções A e C são as respostas corretas.\n",
    "\n",
    "Exemplo 2\n",
    "Pergunta: Crie uma pergunta de múltipla escolha sobre o Renascimento, com mais de uma alternativa correta, obrigatoriamente.\n",
    "Demonstração: \n",
    "\tQuestão:\n",
    "\tDurante o Renascimento, qual ou quais das seguintes obras de arte foi considerada uma das mais importantes da época?\n",
    "\t(A) Mona Lisa (B) The Last Supper (C) The Creation of Adam (D) The Birth of Venus\n",
    "\tAlternativa correta: A e D.\n",
    "\tExplicação: A Mona Lisa, pintada por Leonardo da Vinci, e The Birth of Venus, pintada por Sandro Botticelli, são ambas obras de arte que foram consideradas como algumas das mais importantes do Renascimento. Ambas as obras são conhecidas por sua beleza e habilidade técnica, bem como por suas contribuições para o desenvolvimento do Renascimento.\n",
    "\n",
    "Exemplo 3    \n",
    "Pergunta: Crie uma pergunta de múltipla escolha sobre o Renascimento, com mais de uma alternativa correta, obrigatoriamente.\n",
    "Demonstração: \n",
    "\tQuestão:\n",
    "\tDurante o Renascimento, qual ou quais das seguintes invenções tiveram um impacto significativo na disseminação de ideias e conhecimento?\n",
    "\t(A) Imprensa móvel (B) Gravura em madeira (C) Telescópio (D) Microscópio\n",
    "\tAlternativas corretas: A e B.\n",
    "\tExplicação: A invenção da imprensa móvel, por Johannes Gutenberg, em 1440, permitiu a produção de livros em massa, facilitando a disseminação de ideias e conhecimento. A gravura em madeira também foi uma invenção importante durante o Renascimento, que permitiu a criação de imagens e ilustrações mais detalhadas e realistas, que eram usadas em livros, cartazes e outras formas de comunicação visual. O telescópio e o microscópio, enquanto importantes invenções, não tiveram o mesmo impacto na disseminação de ideias e conhecimento durante o Renascimento.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Suponha que você seja um professor de história com mais de 30 anos de experiência docente.\n",
    "Instruções: Crie uma Questão de múltipla escolha para uma prova de história.\n",
    "Relevância: A questão deveria estar relacionada ao Renascimento.\n",
    "Restrição: Você deve criar uma NOVA Questão (diferente da Demonstração e também com 2 alternativas corretas), suas Alternativas corretas e Explicação.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=512,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Agrupamento: \n",
      " 1.   \"As   vendas   de   carros   elétricos   da   Tesla   continuam   a   subir   apesar   da   pandemia\" \n",
      " 2.   \"Novo   estudo   sugere   que   o   consumo   de   café   pode   diminuir   o   risco   de   doenças   cardacas\" \n",
      " 3.   \"O   modelo   mais   recente   do   iPhone   apresenta   uma   tela   maior   e   câmera   aprimorada\" \n",
      " 4.   \"Casos   de   COVID-19   aumentam   na   ndia,   sobrecarregando   o   sistema   de   sade\" \n",
      " 5.   \"Amazon   anuncia   planos   para   construir   um   novo   centro   de   distribuiço   no   Texas\" \n",
      " 6.   \"Cientistas   descobrem   novas   espécies   de   aves   na   floresta   amazônica\" \n",
      " 7.   \"Conferência   global   sobre   mudanças   climáticas   será   realizada   em   Paris   no   próximo   mês\" \n",
      " 8.   \"Starbucks   apresenta   opçes   de   leite   vegetal   em   todas   as   lojas   dos   EUA\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Categorização e Clustering / Agrupamento #################\n",
    "\n",
    "context = \"\"\"\n",
    "Frases para Agrupar:\n",
    "1. \"A produção industrial está em alta este mês.\"\n",
    "2. \"Agricultores relatam uma colheita abundante nesta temporada.\"\n",
    "3. \"O crescimento econômico acelerou no último trimestre.\"\n",
    "\n",
    "Agrupamento:\n",
    "    Frases 1 e 3: Ambas tratam de crescimento econômico.\n",
    "    Frase 2: Trata de agricultura, não se encaixa diretamente com as outras duas sobre economia.\n",
    "\n",
    "Frases para Agrupar:\n",
    "1. \"Pesquisadores descobrem um novo tipo de antibiótico no solo da floresta tropical.\"\n",
    "2. \"A conferência de tecnologia revela inovações em inteligência artificial.\"\n",
    "3. \"Novo método de reciclagem aumenta a eficiência na conversão de resíduos plásticos.\"\n",
    "\n",
    "Agrupamento:\n",
    "    Frase 1: Relacionada a descobertas científicas.\n",
    "    Frases 2 e 3: Relacionadas a inovações tecnológicas.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Frases para Agrupar:\n",
    "\n",
    "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
    "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
    "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
    "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
    "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
    "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
    "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
    "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
    "\n",
    "Pergunta:\n",
    "Esta é uma tarefa de agrupamento de frases similares. Agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade semântica entre elas.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=512,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################## SUMARIZAÇÃO DE TEXTO ##################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta:   O   texto   é   uma   espiritualidade   e   aborda   a   importância   de   a   pipa   em   nosso   coraço,   a   envergonha   na   juventude   e   a   esforço   para   compensar   essas   coisas   na   vida.   A   vida   no   é   apenas   fazer   coisas   que   valem   a   pena,   mas   também   é   importante   valorizar   o   que   temos   e   ajudar   a   manter   a   beleza   e   a   amizade   da   infância. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# \"World Knowledge\" e Compreensão #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto: \"A água é essencial para a vida na Terra. Ela não apenas sustenta os organismos vivos, mas também participa de vários processos vitais, como o ciclo hidrológico, que afeta o clima global e as condições meteorológicas.\"\n",
    "Pergunta: Resuma o texto.\n",
    "Resposta: A água é crucial para a vida e processos globais como o clima.\n",
    "\n",
    "Texto: \"As florestas tropicais são ricas em biodiversidade. Elas não só abrigam uma grande variedade de espécies animais e vegetais, mas também ajudam a regular o clima terrestre e são vitais para a manutenção de muitos ecossistemas.\"\n",
    "Pergunta: Resuma o texto.\n",
    "Resposta: Florestas tropicais têm alta biodiversidade e são importantes para o clima e ecossistemas.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "Texto: \"Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
    "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
    "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\"\n",
    "\n",
    "Pergunta: Resuma o texto.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta:   Fernando   Pessoa   argumenta   que   a   literatura   é   uma   forma   de   ignorar   a   vida   e   que   a   msica,   artes   visuais   e   artes   vivas   no   se   afastam   da   vida.   A   literatura   simula   a   vida   e   é   uma   forma   de   expresar   ideias   ou   sentimentos   em   linguagem   que   ninguém   emprega. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Sumarização de Texto #################\n",
    "################# \"World Knowledge\" e Compreensão #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto: \"A água é vital para todas as formas de vida conhecidas. A importância da água para os seres vivos é tão grande que ela é considerada uma substância essencial para a existência de vida.\"\n",
    "Pergunta: Resuma o texto.\n",
    "Resposta: A água é crucial para a vida de todos os seres vivos conhecidos.\n",
    "\n",
    "Texto: \"As florestas são essenciais para a saúde do nosso planeta. Elas fornecem oxigênio, abrigam biodiversidade, regulam o clima e oferecem recursos para a vida humana.\"\n",
    "Pergunta: Resuma o texto.\n",
    "Resposta: As florestas desempenham um papel vital no bem-estar do planeta e no suporte à vida.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Texto:  \"Escrever é esquecer. A literatura é a maneira mais agradável de ignorar a vida. \n",
    "A música embala, as artes visuais animam, as artes vivas (como a dança e a arte de representar) entretêm. \n",
    "A primeira, porém, afasta-se da vida por fazer dela um sono; as segundas, contudo, não se afastam da vida - umas porque usam de fórmulas visíveis e portanto vitais, outras porque vivem da mesma vida humana.\n",
    "Não é o caso da literatura. Essa simula a vida. Um romance é uma história do que nunca foi e um drama é um romance dado sem narrativa. \n",
    "Um poema é a expressão de ideias ou de sentimentos em linguagem que ninguém emprega, pois que ninguém fala em verso.\"\n",
    "Fernando Pessoa\n",
    "\n",
    "Pergunta: Resuma o texto.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################### TRANSFORMAÇÃO DE FORMATOS  ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dessa   forma,   os   pesquisadores   continuam   a   defender   as   suas   reivindicaçes. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Converter uma frase na terceira pessoa  para a primeira pessoa #################\n",
    "\n",
    "context = \"\"\"\n",
    "Transforme a frase, que está na terceira pessoa, em frase em primeira pessoa: Silva acredita em outra coisa. De acordo com sua pesquisa, as declarações anteriores sobre este assunto estão incorretas.\n",
    "Resposta: Acredito em outra coisa, disse Silva. De acordo com a minha pesquisa, as declarações anteriores sobre este assunto estão incorretas.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Transforme a frase, que está na terceira pessoa, em frase em primeira pessoa:\n",
    "Apesar dos desafios envolvidos, os pesquisadores continuam a defender as suas reivindicações.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=100,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O   livro   \"Tales   de   Shakespeare\"   é   um   livro   de   jovens   inglês   escrita   por   Charles   Lamb   e   sua   irm   Mary   Lamb   em   1807.   O   livro   está   desenvolvido   para   fazer   familiarizaçes   das   histórias   dos   ttulos   de   Shakespeare   para   os   jovens.   Mas,   como   notado   na   Prefeitura   do   autor,   \"[As   palavras   de   Shakespeare]   so   usadas   qualquer   vez   que   parece   possvel   a   entrar   em   elas;   e   em   qualquer   coisa   que   a   adiciona   para   dizer   a\n"
     ]
    }
   ],
   "source": [
    "################# Machine Translation #################\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "Tales from Shakespeare is an English children's book written by Charles Lamb and his sister Mary Lamb in 1807. The book is designed to make the stories of Shakespeare's plays familiar to the young. However, as noted in the author's Preface, \"[Shakespeare's] words are used whenever it seemed possible to bring them in; and in whatever has been added to give them the regular form of a connected story, diligent care has been taken to select such words as might least interrupt the effect of the beautiful English tongue in which he wrote: therefore, words introduced into our language since his time have been as far as possible avoided.\" \n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Traduza o parágrafo abaixo para o português brasileiro\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Com   o   Vendor,   um   vendado   de   encyclopedias,   aproximou   os   terrenos   em   que   a   casa   do   Hermit,   o   hermito,   estava   situada,   ele   viu   um   signo   que   disse:   \"No   vendado.   Os   trespassantes   sero   prosecutados.   Procédê-lo   com   seu   próprio   risco.\" \n",
      " Além   disso,   o   Vendor   no   tinha   sido   invitado   a   entrar,   ignorou   o   signo   e   vou   a   entrar   na   va   para   a   casa.   Quando   roubou   uma   curva,   uma   poderosa   carga   de  \n"
     ]
    }
   ],
   "source": [
    "################# TRANSORMAÇÃO DE FORMATOS  #################\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "As Seller, an encyclopedia salesman, approached the grounds on which the house of Hermit, the hermit, was situated, he saw a sign that said, \"No salesmen. Trespassers will be prosecuted. Proceed at your own risk.\"\n",
    "Although Seller had not been invited to enter, he ignored the sign and drove up the driveway toward the house. As he rounded a curve, a powerful explosive charge buried in the driveway exploded, and Seller was injured. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Traduza o parágrafo para o português.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A   alternativa   correta   é   (A)   No,   em   caso   Hermit,   quando   ela   plantou   a   charge,   intencionou   apenas   deterr,   no   ajudar,   intrudores. \n",
      " Justificativa:   A   escolha   correta   é   (A)   No,   em   caso   Hermit,   quando   ela   plantou   a   charge,   intencionou   apenas   deterr,   no   ajudar,   intrudores.   A   escolha   correta   é   (A)   No,   em   caso   Hermit,   quando   ela   plantou   a   charge,   intencionou   apenas   deterr,   no   ajudar,   intrudores. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# TRANSORMAÇÃO DE FORMATOS E ICL com self-reflection  #################\n",
    "\n",
    "context = \"\"\"\n",
    "\"Um pedestre atravessou a rua fora da faixa de pedestres e foi atingido por um carro. O motorista estava acima do limite de velocidade.\"\n",
    "Pergunta: O motorista é responsável pelos ferimentos do pedestre?\n",
    "(A) Sim, porque ele estava acima do limite de velocidade.\n",
    "(B) Não, porque o pedestre não estava na faixa.\n",
    "Resposta: (A) Sim, porque ele estava acima do limite de velocidade.\n",
    "Justificativa: Apesar de o pedestre não estar na faixa, o motorista tem a responsabilidade final de controlar a velocidade do veículo para evitar acidentes, o que é uma expectativa legal padrão.\n",
    "\n",
    "\"Um cliente escorrega e cai em uma loja onde um sinal de 'Piso Molhado' estava claramente visível.\"\n",
    "Pergunta: A loja é responsável pelos ferimentos do cliente?\n",
    "(A) Sim, porque é responsabilidade da loja manter um ambiente seguro.\n",
    "(B) Não, porque o sinal de aviso estava visível.\n",
    "Resposta: (B) Não, porque o sinal de aviso estava visível.\n",
    "Justificativa: A presença de um aviso claro reduz a responsabilidade da loja, pois isso significa que o cliente foi devidamente informado sobre o perigo potencial.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Can Seller recover damages from Hermit for his injuries?\n",
    "(A) Yes, unless Hermit, when he planted the charge, intended only to deter, not harm, intruders.\n",
    "(B) Yes, if Hermit was responsible for the explosive charge under the driveway.\n",
    "(C) No, because Seller ignored the sign, which warned him against proceeding further.\n",
    "(D) No, if Hermit reasonably feared that intruders would come and harm him or his family.\n",
    "\n",
    "Justifique a escolha da alternativa correta entre as quatro apresentadas (A, B, C ou D).\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: \n",
      " D.   Pedro   II   passava   os   dias   estudando,[34]   com   apenas   duas   horas   livres   para   recreaço.[35]   Acordava   às   06h30   da   manh   e   começava   seus   estudos   às   sete,   continuando   até   as   dez   da   noite,   quando   ia   para   cama.[36]   Tomou-se   grande   cuidado   em   sua   educaço   para   formar   valores   e   personalidade   diferente   da   impulsividade   e   irresponsabilidade   demonstradas   por   seu   pai.[31][37] \n",
      " Sua   paixo   pela   leitura   lhe   permitiu   assimilar   qualquer   informaço\n"
     ]
    }
   ],
   "source": [
    "################# Retirar Referências do Texto #################\n",
    "\n",
    "context = \"\"\"\n",
    "\n",
    "Esta é uma tarefa de retirada de referências bibliográficas do texto. As referências estão escritas no formato \"[número]\". Exemplo: [10].\n",
    "\n",
    "Exemplos:\n",
    "Contexto: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
    "Resposta: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
    "Contexto: Ontem foi dia de ramos [32].\n",
    "Resposta: Ontem foi dia de ramos.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "Contexto: José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35] Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38] D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41]\n",
    "\n",
    "Remova do texeto as referências no formato \"[número]\" e escreva a Resposta:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################# EXTRAÇÃO DE CONTEÚDO  #############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A   Grande   Barreira   de   Corais   é   o   maior   sistema   de   recifes   de   corais   do   mundo,   composto   por   mais   de   2.900   recifes   individuais   e   900   ilhas   que   se   estendem   por   mais   de   2.300   quilômetros.   Está   localizado   no   Mar   de   Coral,   na   costa   da   Austrália. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Extração de Conteúdo #################\n",
    "\n",
    "context = \"\"\"\n",
    "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem \\\n",
    "por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. \\\n",
    "O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
    "\n",
    "Pergunta: Qual é o maior sistema de recifes de corais do mundo?\n",
    "Resposta: A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo.\n",
    "\n",
    "Pergunta: Quantas espécies de peixes podem ser encontradas na Grande Barreira de Corais?\n",
    "Resposta: Mais de 1.500 espécies de peixes podem ser encontradas na Grande Barreira de Corais.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Dada a descrição do contexto acima:\n",
    "O que é a Grande Barreira de Corais e onde ela está localizada?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=160,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta:   o   quinto   maior   pas   do   mundo   em   território,   com   uma   área   total   de   8.515.767   quilômetros   quadrados. \n",
      " Principal   caracterstica   natural   mencionada   no   texto   é   a   floresta   Amazônica,   que   é   a   maior   floresta   tropical   do   mundo. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Reconhecimento e Extração de Informações Específicas #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto de Referência:\n",
    "O Brasil é o quinto maior país do mundo em território, com uma área total de 8.515.767 quilômetros quadrados. É também o quinto país mais populoso do mundo, \\\n",
    "com uma população estimada em cerca de 211 milhões de pessoas em 2021. \\\n",
    "O país é conhecido por sua diversidade cultural e natural, incluindo a floresta Amazônica, que é a maior floresta tropical do mundo.\n",
    "Pergunta: Qual a dimensão do Brasil em área?\n",
    "Resposta: 8.515.767 quilômetros quadrados.\n",
    "\n",
    "\"\"\"\n",
    "question = \"\"\"\n",
    "Texto de Referência:\n",
    "O Brasil é o quinto maior país do mundo em território, com uma área total de 8.515.767 quilômetros quadrados. É também o quinto país mais populoso do mundo, com uma população estimada em cerca de 211 milhões de pessoas em 2021. O país é conhecido por sua diversidade cultural e natural, incluindo a floresta Amazônica, que é a maior floresta tropical do mundo.\n",
    "Pergunta: Qual é a posição do Brasil em termos de tamanho de território e população, e qual é a principal característica natural mencionada no texto?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intenço:   Solicitar   assistência   para   atualizar   o   carto   de   crédito. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"Estou tendo problemas para fazer login na minha conta, e já tentei redefinir minha senha várias vezes sem sucesso. O que mais eu posso fazer para resolver isso?\"\n",
    "Intenção: Solicitar assistência para reaver acesso à conta.\n",
    "\n",
    "Texto:\n",
    "\"Acabei de receber o produto que comprei de vocês, e estou muito satisfeito com a qualidade. Quero agradecer e dizer que continuarei sendo um cliente fiel.\"\n",
    "Intenção: Expressar satisfação e gratidão.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Olá\n",
    "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
    "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
    "Como posso atualizar meu cartão de crédito?\n",
    "Desde já, obrigado,\n",
    "João \n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "` { \"entities\":   [\"Tim   Cook\",   \"CEO\",   \"Apple\",   \"Steve   Jobs   Theatre\",   \"Cupertino,   Califórnia\",   \"iPhone   15\",   \"várias   cores\",   \"processador   mais   rápido\",   \"maior   duraço   da   bateria\"] } ` `\n"
     ]
    }
   ],
   "source": [
    "################# Reconhecimento e Extração de Entidades #################\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"A Microsoft, fundada por Bill Gates e Paul Allen, está sediada em Redmond, Washington.\"\n",
    "\n",
    "Análise:\n",
    "{\n",
    "  \"pessoa\": [\"Bill Gates\", \"Paul Allen\"],\n",
    "  \"lugar\": [\"Redmond\", \"Washington\"],\n",
    "  \"organização\": [\"Microsoft\"]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
    "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
    "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
    "\n",
    "Analise o parágrafo, identifique em formato JSON Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Entidades: \n",
      " *   Elon   Musk \n",
      " *   SpaceX \n",
      " *   Reduzir   os   custos   de   transporte   espacial \n",
      " *   Possibilitar   a   colonizaço   de   Marte \n",
      " Relacionamentos: \n",
      " *   Elon   Musk   fundou   SpaceX \n",
      " *   SpaceX   reduziu   os   custos   de   transporte   espacial \n",
      " *   SpaceX   possibilitar   a   colonizaço   de   Marte \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Extração de Relações #################\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"Mark Zuckerberg criou o Facebook enquanto estudava em Harvard.\"\n",
    "Análise de Relacionamento:\n",
    "{\n",
    "  \"entidade1\": \"Mark Zuckerberg\",\n",
    "  \"entidade2\": \"Facebook\",\n",
    "  \"relacionamento\": \"criou\"\n",
    "}\n",
    "\n",
    "Texto:\n",
    "\"Jeff Bezos fundou a Amazon em 1994 e transformou-a numa das maiores empresas de comércio eletrônico do mundo.\"\n",
    "Análise de Relacionamento:\n",
    "{\n",
    "  \"entidade1\": \"Jeff Bezos\",\n",
    "  \"entidade2\": \"Amazon\",\n",
    "  \"relacionamento\": \"fundou\"\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Elon Musk fundou a SpaceX em 2002 com o objetivo de reduzir os custos de transporte espacial e possibilitar a colonização de Marte.\n",
    "\n",
    "Analise a frase e extraia o relacionamento entre duas entidades e, em seguida, produza os resultados indicando: entities e relatiohships entre estas no formato JSON\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
