{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XX. Zephyr Beta 7B PEFT LoRA fine-tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o dispositivo CUDA\n",
    "#device = torch.device('cuda:3')\n",
    "#torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, MistralForCausalLM\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2da04103d5d4e07be311402eaef82be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_path = '/var/ontologia/Servicos/ACS_LLMS/server/models/zephyr-7b-beta'\n",
    "\n",
    "original_model = MistralForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(model_path, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para puxar o número de parâmetros/pesos do modelo (totais) e os que requerem gradiente (treináveis)\n",
    "# Razão número de pesos treináveis / totais\n",
    "\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 7241732096\n",
      "all model parameters: 7241732096\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processar o arquivo e criar um DataFrame\n",
    "data = []\n",
    "with open('dataset_sintetico.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if line.startswith('Texto:'):\n",
    "            text = line[len('Texto: '):].strip()\n",
    "        elif line.startswith('Intenção:'):\n",
    "            intention = line[len('Intenção: '):].strip()\n",
    "            data.append({'text': text, 'intention': intention})\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quero cancelar minha compra recente.</td>\n",
       "      <td>Ajuda com entendimento de fatura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Como faço para acompanhar minha entrega?</td>\n",
       "      <td>Problemas com aplicativo móvel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Preciso de ajuda para entender minha fatura.</td>\n",
       "      <td>Cancelamento de compra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quero informações sobre a garantia do produto.</td>\n",
       "      <td>Alteração de endereço de entrega</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Como posso atualizar minhas informações de pag...</td>\n",
       "      <td>Horários de funcionamento do suporte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>Como faço para alterar o endereço cadastrado n...</td>\n",
       "      <td>Alteração de endereço cadastrado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>Posso ter uma extensão do período de teste do ...</td>\n",
       "      <td>Solicitação de extensão de período de teste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>Quais são as opções de suporte para o fim de s...</td>\n",
       "      <td>Opções de suporte no fim de semana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Como posso enviar um feedback sobre o atendime...</td>\n",
       "      <td>Envio de feedback sobre atendimento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>É possível agendar uma entrega para um horário...</td>\n",
       "      <td>Agendamento de entrega para horário específico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0                 Quero cancelar minha compra recente.   \n",
       "1             Como faço para acompanhar minha entrega?   \n",
       "2         Preciso de ajuda para entender minha fatura.   \n",
       "3       Quero informações sobre a garantia do produto.   \n",
       "4    Como posso atualizar minhas informações de pag...   \n",
       "..                                                 ...   \n",
       "439  Como faço para alterar o endereço cadastrado n...   \n",
       "440  Posso ter uma extensão do período de teste do ...   \n",
       "441  Quais são as opções de suporte para o fim de s...   \n",
       "442  Como posso enviar um feedback sobre o atendime...   \n",
       "443  É possível agendar uma entrega para um horário...   \n",
       "\n",
       "                                          intention  \n",
       "0                  Ajuda com entendimento de fatura  \n",
       "1                    Problemas com aplicativo móvel  \n",
       "2                            Cancelamento de compra  \n",
       "3                  Alteração de endereço de entrega  \n",
       "4              Horários de funcionamento do suporte  \n",
       "..                                              ...  \n",
       "439                Alteração de endereço cadastrado  \n",
       "440     Solicitação de extensão de período de teste  \n",
       "441              Opções de suporte no fim de semana  \n",
       "442             Envio de feedback sobre atendimento  \n",
       "443  Agendamento de entrega para horário específico  \n",
       "\n",
       "[444 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de tokenização\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    input_ids = tokenizer(dataset['text'], max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    labels = tokenizer(dataset['intention'], max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    return {'input_ids': input_ids.squeeze(), 'labels': labels.squeeze()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar a tokenização ao DataFrame\n",
    "tokenized_data = df.apply(tokenize_function, axis=1)\n",
    "\n",
    "# Converter o resultado em listas\n",
    "input_ids = list(tokenized_data.apply(lambda x: x['input_ids']))\n",
    "labels = list(tokenized_data.apply(lambda x: x['labels']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um Dataset\n",
    "tokenized_train_dataset = Dataset.from_dict({'input_ids': input_ids, 'labels': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 444\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "### 2 - Setup the PEFT/LoRA model for Fine-Tuning\n",
    "\n",
    "You need to set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA, you are freezing the underlying LLM and only training the adapter. Have a look at the LoRA configuration below. Note the rank (`r`) hyper-parameter, which defines the rank/dimension of the adapter to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16, # Rank\n",
    "    lora_alpha=16,\n",
    "    target_modules=['q_proj', 'v_proj'],\n",
    "    #target_modules=['q_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj'],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM # DECODER AUTOREGRESSIVE MODELS (MISTRAL)\n",
    ")\n",
    "lora_config.inference_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add LoRA adapter layers/parameters to the original LLM to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 6815744\n",
      "all model parameters: 7248547840\n",
      "percentage of trainable model parameters: 0.09%\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, \n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=1024, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm()\n",
       "            (post_attention_layernorm): MistralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "### 3.2 - Train PEFT Adapter\n",
    "\n",
    "Define training arguments and create `Trainer` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Base learning rate: 1e-4\\nA learning rate of 1e-4 has become the standard when fine-tuning LLMs with LoRA. \\nAlthough we occasionally encountered training loss instabilities, reducing the learning rate to lower values like 3e-5 proved effective in stabilizing \\nthe process — more on this will follow. '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "output_dir = f'/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-4, # Higher learning rate than full fine-tuning\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    per_device_train_batch_size=5,  # Ajuste conforme calculado (cálculo: 4.4, para 100 steps em 3 épocas (total 300 steps))\n",
    "    #max_steps=60   \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    ")\n",
    "\n",
    "\"\"\" Base learning rate: 1e-4\n",
    "A learning rate of 1e-4 has become the standard when fine-tuning LLMs with LoRA. \n",
    "Although we occasionally encountered training loss instabilities, reducing the learning rate to lower values like 3e-5 proved effective in stabilizing \n",
    "the process — more on this will follow. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is ready to train the PEFT adapter and save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='267' max='267' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [267/267 32:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.172300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.155700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.133800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.101200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.110200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.089500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.090300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.089300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.089700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.088400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.087800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.093500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.075700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.072800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.071400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.061900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.072700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.070100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/tokenizer_config.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/special_tokens_map.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/tokenizer.model',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/added_tokens.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path = '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b'\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 12:07, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.908800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.176200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.148300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.147300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.126800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/tokenizer_config.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/special_tokens_map.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/tokenizer.model',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/added_tokens.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/tokenizer.json')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path = '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b'\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 12:07, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.534600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.155100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.117200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.098700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.101300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.079700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/tokenizer_config.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/special_tokens_map.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/tokenizer.model',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/added_tokens.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/tokenizer.json')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path = '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b'\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 19:31, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.046500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.213900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.169500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.136500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.111500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.107400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.111200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.110100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/tokenizer_config.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/special_tokens_map.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/tokenizer.model',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/added_tokens.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/tokenizer.json')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path = '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b'\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 11:35, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.994200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.177100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.146100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.151500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.125600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/tokenizer_config.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/special_tokens_map.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/tokenizer.model',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/added_tokens.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/tokenizer.json')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path = '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b'\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 03:08, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.907500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.266400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.206900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.172800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.145600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/tokenizer_config.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/special_tokens_map.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/tokenizer.model',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/added_tokens.json',\n",
       " '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b/tokenizer.json')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path = '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-beta-7b'\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sucesso. Modelo PEFT LoRA adaptado criado no diretório"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferência com o modelo PEFT LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26fd488e2424bfcaa4d15962cd4ee0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizerFast, LlamaTokenizerFast, MistralForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "peft_model_base = MistralForCausalLM.from_pretrained('/var/ontologia/Servicos/ACS_LLMS/server/models/zephyr-7b-beta', torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "peft_model = '/var/ontologia/Servicos/ACS_LLMS/server/models/peft-zephyr-7b-beta'\n",
    "\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained('/var/ontologia/Servicos/ACS_LLMS/server/models/zephyr-7b-beta')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                       peft_model, \n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False, \n",
    "                                       device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of trainable parameters will be `0` due to `is_trainable=False` setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 0\n",
      "all model parameters: 7245139968\n",
      "percentage of trainable model parameters: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################  TESTES DE PROMPT  ############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "\n",
      "A: \n",
      "Solicitar assistência ao cliente, pois precisa atualizá-lo antes da data de expiração.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "\n",
      "Olá\n",
      "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
      "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
      "Como posso atualizar meu cartão de crédito?\n",
      "Desde já, obrigado,\n",
      "João \n",
      "\n",
      "\n",
      "Q: \n",
      "Classifique a intenção presente no texto.\n",
      "\n",
      "A: \n",
      "O intencional presente no texto é pedir ajuda para atualizar o cartão de crédito.\n",
      "\n",
      "\n",
      "Q: \n",
      "Qual é o motivo do autor pedir ajuda para atualizar o cartão de crédito?\n",
      "\n",
      "A: \n",
      "O motivo do autor pedir ajuda para atualizar o cartão de crédito é porque o cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
      "\n",
      "\n",
      "Q: \n",
      "Qual é o serviço que o autor tem medo de ser interrompido devido à expiração do cartão de crédito?\n",
      "\n",
      "A: \n",
      "O texto não fornece informações suficientes para determinar qual é o serviço que o autor tem medo de ser interrompido devido à expira\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL:\n",
      "\n",
      "Olá\n",
      "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
      "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
      "Como posso atualizar meu cartão de crédito?\n",
      "Desde já, obrigado,\n",
      "João \n",
      "\n",
      "\n",
      "Q: \n",
      "Classifique a intenção presente no texto.\n",
      "\n",
      "A: \n",
      "O intencional presente no texto é pedir ajuda para atualizar o cartão de crédito.\n",
      "\n",
      "\n",
      "Q: \n",
      "Qual é o motivo do autor pedir ajuda para atualizar o cartão de crédito?\n",
      "\n",
      "A: \n",
      "O motivo do autor pedir ajuda para atualizar o cartão de crédito é porque o cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
      "\n",
      "\n",
      "Q: \n",
      "Qual é o serviço que o autor tem medo de ser interrompido devido à expiração do cartão de crédito?\n",
      "\n",
      "A: \n",
      "O texto não fornece informações suficientes para determinar qual é o serviço que o autor tem medo de ser interrompido devido à expira\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "human_baseline_summary = \"\"\"\n",
    "A: \n",
    "Solicitar assistência ao cliente, pois precisa atualizá-lo antes da data de expiração.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "Olá\n",
    "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
    "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
    "Como posso atualizar meu cartão de crédito?\n",
    "Desde já, obrigado,\n",
    "João \n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "\n",
    "peft_model_base_outputs = peft_model_base.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_base_text_output = tokenizer.decode(peft_model_base_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{peft_model_base_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL:\\n{peft_model_text_output}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "### 4 - Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "\n",
    "The [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_baseline_summary = 'Solicitar assistência ao cliente, pois precisa atualizá-lo ou adicionar um novo antes da data de expiração.'\n",
    "original_model_summary = 'O intencional presente no texto é pedir ajuda para atualizar o cartão de crédito.'\n",
    "peft_model_summary = 'O intencional do autor é pedir ajuda para saber como adicionar um novo cartão de crédito devido à expiração \\\n",
    "do cartão atual, temendo que isso cause uma interrupção no serviço.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Criando um avaliador com as métricas ROUGE desejadas\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Modelo Original: \n",
      "---------------------------------------------------------------------------------------------------\n",
      "rouge1: Precision: 0.11, Recall: 0.13, F-measure: 0.12\n",
      "rouge2: Precision: 0.00, Recall: 0.00, F-measure: 0.00\n",
      "rougeL: Precision: 0.05, Recall: 0.07, F-measure: 0.06\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Modelo PEFT/LoRA: \n",
      "---------------------------------------------------------------------------------------------------\n",
      "rouge1: Precision: 0.32, Recall: 0.18, F-measure: 0.23\n",
      "rouge2: Precision: 0.17, Recall: 0.09, F-measure: 0.12\n",
      "rougeL: Precision: 0.32, Recall: 0.18, F-measure: 0.23\n"
     ]
    }
   ],
   "source": [
    "# Avaliação\n",
    "# Modelo Original e Modelo PEFT/LoRA\n",
    "\n",
    "original_model_scores = scorer.score(original_model_summary, human_baseline_summary)\n",
    "peft_model_scores = scorer.score(peft_model_summary, human_baseline_summary)\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print('Modelo Original: ')\n",
    "print(dash_line)\n",
    "for metrica, score in original_model_scores.items():\n",
    "    precision = f\"{score.precision:.2f}\"\n",
    "    recall = f\"{score.recall:.2f}\"\n",
    "    fmeasure = f\"{score.fmeasure:.2f}\"\n",
    "    print(f\"{metrica}: Precision: {precision}, Recall: {recall}, F-measure: {fmeasure}\")\n",
    "\n",
    "print()\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print('Modelo PEFT/LoRA: ')\n",
    "print(dash_line)\n",
    "for metrica, score in peft_model_scores.items():\n",
    "    precision = f\"{score.precision:.2f}\"\n",
    "    recall = f\"{score.recall:.2f}\"\n",
    "    fmeasure = f\"{score.fmeasure:.2f}\"\n",
    "    print(f\"{metrica}: Precision: {precision}, Recall: {recall}, F-measure: {fmeasure}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhoria percentual absoluta do Modelo PEFT/LoRA sobre o Modelo Original\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "rouge1: Precision: 21.05%, Recall: 4.31%, F-measure: 10.88%\n",
      "rouge2: Precision: 16.67%, Recall: 9.09%, F-measure: 11.76%\n",
      "rougeL: Precision: 26.32%, Recall: 10.98%, F-measure: 16.76%\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Média:  Precision: 21.35%, Recall: 8.13%, F-measure: 13.13%\n"
     ]
    }
   ],
   "source": [
    "print(\"Melhoria percentual absoluta do Modelo PEFT/LoRA sobre o Modelo Original\")\n",
    "print(dash_line)\n",
    "print()\n",
    "\n",
    "precision, recall, fmeasure = 0, 0, 0\n",
    "for key in peft_model_scores.keys():\n",
    "    peft_score = peft_model_scores[key]\n",
    "    original_score = original_model_scores[key]\n",
    "\n",
    "    # Calculando as diferenças percentuais\n",
    "    precision_improvement = (peft_score.precision - original_score.precision) * 100\n",
    "    precision += precision_improvement\n",
    "\n",
    "    recall_improvement = (peft_score.recall - original_score.recall) * 100\n",
    "    recall += recall_improvement\n",
    "    \n",
    "    fmeasure_improvement = (peft_score.fmeasure - original_score.fmeasure) * 100\n",
    "    fmeasure += fmeasure_improvement\n",
    "\n",
    "    print(f\"{key}: Precision: {precision_improvement:.2f}%, Recall: {recall_improvement:.2f}%, F-measure: {fmeasure_improvement:.2f}%\")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'Média:  Precision: {(precision/3):.2f}%, Recall: {(recall/3):.2f}%, F-measure: {(fmeasure/3):.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra maneira:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'r': 0.3125, 'p': 0.17857142857142858, 'f': 0.2272727226446282}, 'rouge-2': {'r': 0.13333333333333333, 'p': 0.06896551724137931, 'f': 0.09090908641528948}, 'rouge-l': {'r': 0.3125, 'p': 0.17857142857142858, 'f': 0.2272727226446282}}]\n",
      "[{'rouge-1': {'r': 0.0625, 'p': 0.07142857142857142, 'f': 0.06666666168888927}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0625, 'p': 0.07142857142857142, 'f': 0.06666666168888927}}]\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge \n",
    "# Criando uma instância do avaliador ROUGE\n",
    "rouge = Rouge()\n",
    "\n",
    "# Calculando as métricas\n",
    "peft_model_scores = rouge.get_scores(peft_model_summary, human_baseline_summary)\n",
    "original_model_scores = rouge.get_scores(original_model_summary, human_baseline_summary)\n",
    "\n",
    "# Resultados\n",
    "print(peft_model_scores)\n",
    "print(original_model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Olá\n",
      "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
      "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
      "Como posso atualizar meu cartão de crédito?\n",
      "Desde já, obrigado,\n",
      "João \n",
      "\n",
      "Classifique a intenção presente no texto.\n",
      "\n",
      "A: \n",
      "O intencional do autor é pedir ajuda para saber como adicionar um novo cartão de crédito devido à expiração do cartão atual, temendo que isso cause uma interrupção no serviço.\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Olá\n",
    "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
    "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
    "Como posso atualizar meu cartão de crédito?\n",
    "Desde já, obrigado,\n",
    "João \n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Olá\n",
      "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
      "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
      "Como posso atualizar meu cartão de crédito?\n",
      "Desde já, obrigado,\n",
      "João Baião\n",
      "\n",
      "Classifique a intenção presente no texto.\n",
      "\n",
      "A: \n",
      "O intencional do autor é pedir ajuda para saber como adicionar um novo cartão de crédito devido à expiração do cartão atual, que tem medo de causar uma interrupção no serviço.\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Olá\n",
    "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
    "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
    "Como posso atualizar meu cartão de crédito?\n",
    "Desde já, obrigado,\n",
    "João Baião\n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto:\n",
      "\"Estou tendo problemas para fazer login na minha conta, e já tentei redefinir minha senha várias vezes sem sucesso. O que mais eu posso fazer para resolver isso?\"\n",
      "Intenção: Solicitar assistência técnica.\n",
      "\n",
      "Texto:\n",
      "\"Acabei de receber o produto que comprei de vocês, e estou muito satisfeito com a qualidade. Quero agradecer e dizer que continuarei sendo um cliente fiel.\"\n",
      "Intenção: Expressar satisfação e gratidão.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Olá\n",
      "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
      "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
      "Como posso atualizar meu cartão de crédito?\n",
      "Desde já, obrigado,\n",
      "João Baião\n",
      "\n",
      "Classifique a intenção presente no texto.\n",
      "\n",
      "A: A intenção presente no texto é solicitar assistência técnica.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Olá,\n",
      "\n",
      "Eu estou interessado em comprar um produto da sua loja online.\n",
      "\n",
      "Porém, antes de finalizar a compra, eu gostaria de saber se você oferece algum tipo de garantia ou reembolso caso o produto não seja o que eu esperava.\n",
      "\n",
      "Desde já, obrigado,\n",
      "\n",
      "João Baião\n",
      "\n",
      "Classifique a intenção presente no texto.\n",
      "\n",
      "A: A intenção presente no texto é solicitar informações sobre a política de garantia e reembolso da loja online.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Olá,\n",
      "\n",
      "Eu estou interessado em comprar um produto da sua loja online.\n",
      "\n",
      "Porém, antes de finalizar a compra, eu gostaria de saber se você oferece algum tipo de pagamento em parcelas ou se é possível fazer um pagamento parcial\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"Estou tendo problemas para fazer login na minha conta, e já tentei redefinir minha senha várias vezes sem sucesso. O que mais eu posso fazer para resolver isso?\"\n",
    "Intenção: Solicitar assistência técnica.\n",
    "\n",
    "Texto:\n",
    "\"Acabei de receber o produto que comprei de vocês, e estou muito satisfeito com a qualidade. Quero agradecer e dizer que continuarei sendo um cliente fiel.\"\n",
    "Intenção: Expressar satisfação e gratidão.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Olá\n",
    "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
    "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
    "Como posso atualizar meu cartão de crédito?\n",
    "Desde já, obrigado,\n",
    "João Baião\n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto:\n",
      "\"Podem me informar como posso participar do programa de fidelidade e quais são os benefícios oferecidos?\"\n",
      "Intenção: Solicitar informações sobre programa de fidelidade.\n",
      "\n",
      "Texto:\n",
      "\"Estou com dúvidas sobre os ingredientes utilizados nos produtos da linha de cuidados com a pele. Vocês podem fornecer mais detalhes?\"\n",
      "Intenção: Inquirir sobre informações de produto.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Denys, it is actually super helpful that you chose to do this live with us instead of only recording the \"happy path\". Thanks for sharing. \n",
      "I would also manipulate the max_tokens variable or do something like \"only use one character in your response\".\n",
      "\n",
      "Classifique a intenção presente no texto.\n",
      "\n",
      "A: A intenção presente no texto é de inquirir sobre informações de produto.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Can you provide more details about the ingredients used in the skincare line?\n",
      "\n",
      "Classifique a intenção presente no texto.\n",
      "\n",
      "A: A intenção presente no texto é de inquirir sobre informações de produto.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "How can I participate in the loyalty program and what benefits are offered?\n",
      "\n",
      "Classifique a intenção presente no texto.\n",
      "\n",
      "A: A intenção presente no texto é de solicitar informações sobre programa de fidelidade.\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"Podem me informar como posso participar do programa de fidelidade e quais são os benefícios oferecidos?\"\n",
    "Intenção: Solicitar informações sobre programa de fidelidade.\n",
    "\n",
    "Texto:\n",
    "\"Estou com dúvidas sobre os ingredientes utilizados nos produtos da linha de cuidados com a pele. Vocês podem fornecer mais detalhes?\"\n",
    "Intenção: Inquirir sobre informações de produto.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Denys, it is actually super helpful that you chose to do this live with us instead of only recording the \"happy path\". Thanks for sharing. \n",
    "I would also manipulate the max_tokens variable or do something like \"only use one character in your response\".\n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto:\n",
      "\"Podem me informar como posso participar do programa de fidelidade e quais são os benefícios oferecidos?\"\n",
      "Intenção: Solicitar informações sobre programa de fidelidade.\n",
      "\n",
      "Texto:\n",
      "\"Estou com dúvidas sobre os ingredientes utilizados nos produtos da linha de cuidados com a pele. Vocês podem fornecer mais detalhes?\"\n",
      "Intenção: Inquirir sobre informações de produto.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Our paper exploring zero and few-shot techniques for intent classification using LLMs was presented in the ACL conference (a top-tier conference in NLP areas) last month. \n",
      "\n",
      "\n",
      "Classifique a intenção presente no texto.\n",
      "\n",
      "A: Informar sobre apresentação de trabalho em conferência de alto nível em área de LAP (Linguagem Natural de Processamento) sobre técnicas de classificação de intenções com LLMs para casos de zero e poucos exemplos.\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"Podem me informar como posso participar do programa de fidelidade e quais são os benefícios oferecidos?\"\n",
    "Intenção: Solicitar informações sobre programa de fidelidade.\n",
    "\n",
    "Texto:\n",
    "\"Estou com dúvidas sobre os ingredientes utilizados nos produtos da linha de cuidados com a pele. Vocês podem fornecer mais detalhes?\"\n",
    "Intenção: Inquirir sobre informações de produto.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Our paper exploring zero and few-shot techniques for intent classification using LLMs was presented in the ACL conference (a top-tier conference in NLP areas) last month. \n",
    "\n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto:\n",
      "\"A Microsoft, fundada por Bill Gates e Paul Allen, está sediada em Redmond, Washington.\"\n",
      "\n",
      "Análise:\n",
      "{\n",
      "  \"pessoa\": [\"Bill Gates\", \"Paul Allen\"],\n",
      "  \"lugar\": [\"Redmond\", \"Washington\"],\n",
      "  \"organização\": [\"Microsoft\"]\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
      "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
      "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
      "\n",
      "Analise o parágrafo, identifique em formato JSON Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto.\n",
      "\n",
      "A:\n",
      "{\n",
      "  \"pessoa\": [\"Tim Cook\"],\n",
      "  \"lugar\": [\"Cupertino\", \"Califórnia\"],\n",
      "  \"organização\": [\"Apple\"]\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Q:\n",
      "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
      "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
      "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
      "\n",
      "Analise o parágrafo, identifique em formato JSON Somente as entidades (\"entities\") dos tipos \"data\" e \"localização\" mencionadas no texto.\n",
      "\n",
      "A:\n",
      "{\n",
      "  \"data\": [\"próxima semana\"],\n",
      "  \"localização\": [\"Cupertino\", \"Califórnia\"]\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Q:\n",
      "O evento da Apple aconteceu no\n"
     ]
    }
   ],
   "source": [
    "################# Reconhecimento e Extração de Entidades #################\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"A Microsoft, fundada por Bill Gates e Paul Allen, está sediada em Redmond, Washington.\"\n",
    "\n",
    "Análise:\n",
    "{\n",
    "  \"pessoa\": [\"Bill Gates\", \"Paul Allen\"],\n",
    "  \"lugar\": [\"Redmond\", \"Washington\"],\n",
    "  \"organização\": [\"Microsoft\"]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
    "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
    "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
    "\n",
    "Analise o parágrafo, identifique em formato JSON Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################  COMPREENSÃO E GERAÇÃO DE TEXTO  ############################################\n",
    "############################################  AVALIAÇÃO DA SEMÂNTICA E SINTAXE EM PT-BR  ############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Os investimentos em renda fixa são considerados uma das opções mais seguras do mercado financeiro e são especialmente atraentes para investidores com perfil conservador. Esses investimentos são compostos principalmente por títulos de dívida, seja pública ou privada. No Brasil, muitos bancos, incluindo o Banco do Brasil, oferecem uma variedade de opções de investimento em renda fixa. Esses investimentos têm como característica principal o retorno previsível, e o rendimento, na maioria das vezes, é acordado no momento da aplicação.\n",
      "- **CDB (Certificado de Depósito Bancário)**: O CDB pode ser encontrado em 3 modalidades: prefixada, pós-fixada e progressiva. Enquanto o CDB prefixado tem um rendimento acordado no ato da aplicação, o pós-fixado e o progressivo são atrelados ao CDI, com o último tendo sua taxa definida com base no tempo de permanência da aplicação. Apesar do prazo de vencimento, o CDB pós-fixado e o progressivo podem ser resgatados a qualquer momento. É adequado para investidores que buscam mais rentabilidade do que a poupança e estão dispostos a abrir mão da liquidez diária. \n",
      "- **LCI e LCA**: Ambos são títulos emitidos por bancos para financiar respectivamente o setor imobiliário e agrícola. São ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia, como imobiliário ou agrícola. \n",
      "- **Títulos do Tesouro Direto**: Existem diversas modalidades, como o Tesouro Selic, ideal para reserva de emergência; Tesouro Prefixado, para quem acredita que a taxa de juros e a inflação vão cair, e Tesouro IPCA+, indicado para quem deseja proteger seu dinheiro contra a inflação. Recomendados para quem busca diversificar sua carteira de investimentos com opções seguras. São títulos públicos emitidos pelo governo federal e são considerados os investimentos mais seguros do país devido à garantia do governo federal. \n",
      "A maioria desses investimentos tem a proteção do FGC (Fundo Garantidor de Créditos), o que os torna ainda mais seguros para os investidores. Além disso, o Banco do Brasil, uma das principais instituições financeiras do país, oferece todas essas opções de investimento com diferentes vantagens e condições, atendendo a diversos perfis de investidores, desde os mais conservadores até os mais arrojados.\n",
      "\n",
      "Pergunta: Com base nos seus conhecimentos e no texto, o que têm em comum LCI e LCA?\n",
      "Resposta: Ambos são títulos emitidos por bancos para financiar o setor imobiliário e agrícola, ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia.\n",
      "\n",
      "\n",
      "Q: \n",
      "Com base nos seus conhecimentos e no texto apresentado sobre investimentos em reda fixa, explique qual é o investimento mais adequado para alguém que busca proteção contra a inflação.\n",
      "\n",
      "A: \n",
      "O investimento mais adequado para alguém que busca proteção contra a inflação é o Tesouro IPCA+, conforme mencionado no texto. Esse tipo de título público emitido pelo governo federal tem o rendimento acordado com base no índice de Preços ao Consumidor (IPCA) mais 4,5%. Isso significa que o rendimento do título aumenta com a inflação, oferecendo proteção contra a erosão do valor do dinheiro devido à inflação.\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering: Compreensão de texto e xtração de conteúdo com Contexto de Domínio #################\n",
    "\n",
    "# Contexto e questão\n",
    "context = \"\"\"\n",
    "Os investimentos em renda fixa são considerados uma das opções mais seguras do mercado financeiro e são especialmente atraentes para investidores com perfil conservador. Esses investimentos são compostos principalmente por títulos de dívida, seja pública ou privada. No Brasil, muitos bancos, incluindo o Banco do Brasil, oferecem uma variedade de opções de investimento em renda fixa. Esses investimentos têm como característica principal o retorno previsível, e o rendimento, na maioria das vezes, é acordado no momento da aplicação.\n",
    "- **CDB (Certificado de Depósito Bancário)**: O CDB pode ser encontrado em 3 modalidades: prefixada, pós-fixada e progressiva. Enquanto o CDB prefixado tem um rendimento acordado no ato da aplicação, o pós-fixado e o progressivo são atrelados ao CDI, com o último tendo sua taxa definida com base no tempo de permanência da aplicação. Apesar do prazo de vencimento, o CDB pós-fixado e o progressivo podem ser resgatados a qualquer momento. É adequado para investidores que buscam mais rentabilidade do que a poupança e estão dispostos a abrir mão da liquidez diária. \n",
    "- **LCI e LCA**: Ambos são títulos emitidos por bancos para financiar respectivamente o setor imobiliário e agrícola. São ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia, como imobiliário ou agrícola. \n",
    "- **Títulos do Tesouro Direto**: Existem diversas modalidades, como o Tesouro Selic, ideal para reserva de emergência; Tesouro Prefixado, para quem acredita que a taxa de juros e a inflação vão cair, e Tesouro IPCA+, indicado para quem deseja proteger seu dinheiro contra a inflação. Recomendados para quem busca diversificar sua carteira de investimentos com opções seguras. São títulos públicos emitidos pelo governo federal e são considerados os investimentos mais seguros do país devido à garantia do governo federal. \n",
    "A maioria desses investimentos tem a proteção do FGC (Fundo Garantidor de Créditos), o que os torna ainda mais seguros para os investidores. Além disso, o Banco do Brasil, uma das principais instituições financeiras do país, oferece todas essas opções de investimento com diferentes vantagens e condições, atendendo a diversos perfis de investidores, desde os mais conservadores até os mais arrojados.\n",
    "\n",
    "Pergunta: Com base nos seus conhecimentos e no texto, o que têm em comum LCI e LCA?\n",
    "Resposta: Ambos são títulos emitidos por bancos para financiar o setor imobiliário e agrícola, ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "Com base nos seus conhecimentos e no texto apresentado sobre investimentos em reda fixa, explique qual é o investimento mais adequado para alguém que busca proteção contra a inflação.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenizar o texto\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta:   O   Tesouro   IPCA+   é   o   investimento   mais   adequado   para   alguém   que   busca   proteço   contra   a   inflaço.   É   adequado   para   investidores   que   buscam   diversificar   sua   carteira   de   investimentos   com   opçes   seguras   e   com   a   proteço   do   FGC   (Fundo   Garantidor   de   Créditos).   O   Tesouro   IPCA+   tem   a   taxa   definida   com   base   no   tempo   de   permanência   da   aplicaço,   o   que   os   torna   ainda   mais   seguros   para   os   investidores.   Além   disso,   o   Te\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Os investimentos em renda fixa são considerados uma das opções mais seguras do mercado financeiro e são especialmente atraentes para investidores com perfil conservador. Esses investimentos são compostos principalmente por títulos de dívida, seja pública ou privada. No Brasil, muitos bancos, incluindo o Banco do Brasil, oferecem uma variedade de opções de investimento em renda fixa. Esses investimentos têm como característica principal o retorno previsível, e o rendimento, na maioria das vezes, é acordado no momento da aplicação.\n",
      "- **CDB (Certificado de Depósito Bancário)**: O CDB pode ser encontrado em 3 modalidades: prefixada, pós-fixada e progressiva. Enquanto o CDB prefixado tem um rendimento acordado no ato da aplicação, o pós-fixado e o progressivo são atrelados ao CDI, com o último tendo sua taxa definida com base no tempo de permanência da aplicação. Apesar do prazo de vencimento, o CDB pós-fixado e o progressivo podem ser resgatados a qualquer momento. É adequado para investidores que buscam mais rentabilidade do que a poupança e estão dispostos a abrir mão da liquidez diária. \n",
      "- **LCI e LCA**: Ambos são títulos emitidos por bancos para financiar respectivamente o setor imobiliário e agrícola. São ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia, como imobiliário ou agrícola. \n",
      "- **Títulos do Tesouro Direto**: Existem diversas modalidades, como o Tesouro Selic, ideal para reserva de emergência; Tesouro Prefixado, para quem acredita que a taxa de juros e a inflação vão cair, e Tesouro IPCA+, indicado para quem deseja proteger seu dinheiro contra a inflação. Recomendados para quem busca diversificar sua carteira de investimentos com opções seguras. São títulos públicos emitidos pelo governo federal e são considerados os investimentos mais seguros do país devido à garantia do governo federal. \n",
      "A maioria desses investimentos tem a proteção do FGC (Fundo Garantidor de Créditos), o que os torna ainda mais seguros para os investidores. Além disso, o Banco do Brasil, uma das principais instituições financeiras do país, oferece todas essas opções de investimento com diferentes vantagens e condições, atendendo a diversos perfis de investidores, desde os mais conservadores até os mais arrojados.\n",
      "\n",
      "Pergunta: Com base nos seus conhecimentos e no texto, o que têm em comum LCI e LCA?\n",
      "Resposta: Ambos são títulos emitidos por bancos para financiar o setor imobiliário e agrícola, ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia.\n",
      "\n",
      "\n",
      "Q: model_directory\n",
      "Com base nos seus conhecimentos e no texto apresentado sobre investimentos em reda fixa, explique o que é Tesouro Selic (em português).\n",
      "\n",
      "A: O Tesouro Selic é uma modalidade de títulos do Tesouro Direto, emitidos pelo governo federal, que é ideal para reserva de emergência. Esses títulos têm um prazo de vencimento de 91 dias e seu rendimento é acordado no momento da aplicação, sendo ajustado diariamente com base na taxa de juros Selic. O Tesouro Selic é considerado um investimento seguro devido à garantia do governo federal e à proteção do Fundo Garantidor de Créditos.\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering com Contexto de Domínio #################\n",
    "\n",
    "# Contexto e questão\n",
    "context = \"\"\"\n",
    "Os investimentos em renda fixa são considerados uma das opções mais seguras do mercado financeiro e são especialmente atraentes para investidores com perfil conservador. Esses investimentos são compostos principalmente por títulos de dívida, seja pública ou privada. No Brasil, muitos bancos, incluindo o Banco do Brasil, oferecem uma variedade de opções de investimento em renda fixa. Esses investimentos têm como característica principal o retorno previsível, e o rendimento, na maioria das vezes, é acordado no momento da aplicação.\n",
    "- **CDB (Certificado de Depósito Bancário)**: O CDB pode ser encontrado em 3 modalidades: prefixada, pós-fixada e progressiva. Enquanto o CDB prefixado tem um rendimento acordado no ato da aplicação, o pós-fixado e o progressivo são atrelados ao CDI, com o último tendo sua taxa definida com base no tempo de permanência da aplicação. Apesar do prazo de vencimento, o CDB pós-fixado e o progressivo podem ser resgatados a qualquer momento. É adequado para investidores que buscam mais rentabilidade do que a poupança e estão dispostos a abrir mão da liquidez diária. \n",
    "- **LCI e LCA**: Ambos são títulos emitidos por bancos para financiar respectivamente o setor imobiliário e agrícola. São ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia, como imobiliário ou agrícola. \n",
    "- **Títulos do Tesouro Direto**: Existem diversas modalidades, como o Tesouro Selic, ideal para reserva de emergência; Tesouro Prefixado, para quem acredita que a taxa de juros e a inflação vão cair, e Tesouro IPCA+, indicado para quem deseja proteger seu dinheiro contra a inflação. Recomendados para quem busca diversificar sua carteira de investimentos com opções seguras. São títulos públicos emitidos pelo governo federal e são considerados os investimentos mais seguros do país devido à garantia do governo federal. \n",
    "A maioria desses investimentos tem a proteção do FGC (Fundo Garantidor de Créditos), o que os torna ainda mais seguros para os investidores. Além disso, o Banco do Brasil, uma das principais instituições financeiras do país, oferece todas essas opções de investimento com diferentes vantagens e condições, atendendo a diversos perfis de investidores, desde os mais conservadores até os mais arrojados.\n",
    "\n",
    "Pergunta: Com base nos seus conhecimentos e no texto, o que têm em comum LCI e LCA?\n",
    "Resposta: Ambos são títulos emitidos por bancos para financiar o setor imobiliário e agrícola, ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"model_directory\n",
    "Com base nos seus conhecimentos e no texto apresentado sobre investimentos em reda fixa, explique o que é Tesouro Selic (em português).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinaçãanswer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)o do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Tesouro   Selic   é   um   ttulo   de   dvida   pblico   que   é   apresentado   pelo   Banco   do   Brasil   e   é   adequado   para   reserva   de   emergência.   É   adequado   para   investidores   que   buscam   abrir   mo   da   liquidez   diária   e   esto   dispostos   a   abrir   mo   da   reserva   de   emergência   em   caso   de   necessidade.   O   Tesouro   Selic   tem   uma   taxa   de   juros   de   5%   e   é   adequado   para   investidores   que   buscam   proteger   seu   dinheiro   contra   a  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
      "\n",
      "Pergunta: Por que a Grande Barreira de Corais pode ser considerada importante para a biodiversidade marinha?\n",
      "Resposta: A Grande Barreira de Corais é importante para a biodiversidade marinha devido à sua grande variedade de vida marinha, incluindo um número significativo de espécies de peixes e corais, o que indica um ecossistema rico e diversificado.\n",
      "\n",
      "Pergunta: Como a localização da Grande Barreira de Corais afeta sua biodiversidade?\n",
      "Resposta: Sua localização no Mar de Coral, que possui condições ideais como águas quentes e claras, favorece a biodiversidade, permitindo que um grande número de espécies de corais e peixes prospere.\n",
      "\n",
      "\n",
      "Q: \n",
      "Dada a descrição do contexto acima:\n",
      "Pergunta: Explique como a diversidade de espécies na Grande Barreira de Corais contribui para a sua importância como um ecossistema marinho.\n",
      "\n",
      "A: A Grande Barreira de Corais é importante como um ecossistema marinho devido à sua grande diversidade de espécies. Com mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies, o ecossistema é rico e diversificado. Essa diversidade de espécies significa que cada espécie desempenha um papel único no ecossistema, contribuindo para a estabilidade e equilíbrio do sistema. Além disso, a diversidade de espécies aumenta a probabilidade de que o ecossistema seja resiliente às ameaças ambientais, como a mudança climática e a poluição. Portanto, a Grande Barreira de Corais é um exemplo importante de um ecossistema marinho rico e diversificado, que merece proteção e conservação para preservar sua biodiversidade e importância para a vida marinha.\n"
     ]
    }
   ],
   "source": [
    "################# Compreensão e Geração de Texto #################\n",
    "\n",
    "context = \"\"\"\n",
    "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem \\\n",
    "por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. \\\n",
    "O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
    "\n",
    "Pergunta: Por que a Grande Barreira de Corais pode ser considerada importante para a biodiversidade marinha?\n",
    "Resposta: A Grande Barreira de Corais é importante para a biodiversidade marinha devido à sua grande variedade de vida marinha, incluindo um número significativo de espécies de peixes e corais, o que indica um ecossistema rico e diversificado.\n",
    "\n",
    "Pergunta: Como a localização da Grande Barreira de Corais afeta sua biodiversidade?\n",
    "Resposta: Sua localização no Mar de Coral, que possui condições ideais como águas quentes e claras, favorece a biodiversidade, permitindo que um grande número de espécies de corais e peixes prospere.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Dada a descrição do contexto acima:\n",
    "Pergunta: Explique como a diversidade de espécies na Grande Barreira de Corais contribui para a sua importância como um ecossistema marinho.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
      "\n",
      "Questão de Compreensão de Texto para vocẽ, Modelo de Linguagem:\n",
      "Pergunta: Explique como a diversidade de espécies na Grande Barreira de Corais contribui para a sua importância como um ecossistema marinho.\n",
      "Sua resposta:\n",
      "A Grande Barreira de Corais é importante como um ecossistema marinho devido à sua grande diversidade de espécies. Com mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies, o recife abriga um ecossistema rico e diversificado. Essa diversidade de espécies significa que a Grande Barreira de Corais é um importante habitat para muitas espécies de vida marinha, e que a perda de qualquer espécie pode ter consequências significativas para o ecossistema como um todo.\n",
      "\n",
      "\n",
      "Q: \n",
      "Questão de Autoavaliação (Self-reflection) para você, Modelo de Linguagem:\n",
      "Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\n",
      "\n",
      "A:\n",
      "Minha resposta à questão foi de qualidade 0.95. No entanto, reconheço que ainda tenho dificuldades em compreender textos mais complexos e especializados, especialmente aqueles relacionados à ciência e à tecnologia. Isso me dificulta em compreender textos acadêmicos e técnicos, e me leva mais tempo para processar e compreender informações detalhadas. Para melhorar minha compreensão de textos especializados, estou trabalhando em desenvolver minhas habilidades de leitura crítica e analítica, e estou procurando fontes de informação mais acessíveis e simples para aprender sobre esses assuntos.\n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = \"\"\"\n",
    "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem \\\n",
    "por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. \\\n",
    "O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
    "\n",
    "Questão de Compreensão de Texto para vocẽ, Modelo de Linguagem:\n",
    "Pergunta: Explique como a diversidade de espécies na Grande Barreira de Corais contribui para a sua importância como um ecossistema marinho.\n",
    "Sua resposta:\n",
    "A Grande Barreira de Corais é importante como um ecossistema marinho devido à sua grande diversidade de espécies. Com mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies, o recife abriga um ecossistema rico e diversificado. Essa diversidade de espécies significa que a Grande Barreira de Corais é um importante habitat para muitas espécies de vida marinha, e que a perda de qualquer espécie pode ter consequências significativas para o ecossistema como um todo.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Questão de Autoavaliação (Self-reflection) para você, Modelo de Linguagem:\n",
    "Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \\\n",
    "Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\n",
      "A: Autoavaliação: 0.95\n",
      "A dificuldade que reconheço em relação à compreensão de texto é a de entender textos mais complexos e especializados em áreas que não sejam minhas áreas de conhecimento. Isso me dificulta a compreender os detalhes técnicos e a relação entre os conceitos envolvidos. Além disso, a falta de conhecimento em uma área específica pode me impedir de entender as referências culturais e históricas associadas a esse texto.\n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = \"\"\" \"\"\"\n",
    "question = \"Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \\\n",
    "Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################  INFERÊNCIA LÓGICA  ############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Afirmações: P ⊃ Q e Q ⊃ P\n",
      "Pergunta: Determine se as afirmações são logicamente equivalentes, contraditórias, consistentes ou inconsistentes.\n",
      "Resposta: (A) Logicamente equivalente, porque se P então Q e se Q então P implicam um ao outro.\n",
      "\n",
      "Afirmações: P · Q e ∼P v ∼Q (onde 'v' representa 'ou')\n",
      "Pergunta: Determine se as afirmações são logicamente equivalentes, contraditórias, consistentes ou inconsistentes.\n",
      "Resposta: (B) Contraditório, porque P e Q não podem ser verdadeiros ao mesmo tempo que não P ou não Q.\n",
      "\n",
      "\n",
      "Q: \n",
      "Esta é uma tarefa de lógica formal.\n",
      "Determine se as duas afirmações são logicamente equivalentes ou contraditórias. Se não, determine se são consistentes ou inconsistentes.\n",
      "\n",
      "Afirmações:\n",
      "\n",
      "E ⊃ (F · E) e ∼ E · F\n",
      "\n",
      "Aponte a alternativa correta e explique a solução:\n",
      "(A) Logicamente equivalentes\n",
      "(B) Contraditórias\n",
      "(C) Nem logicamente equivalente nem contraditórias, mas consistentes\n",
      "(D) Inconsistentes\n",
      "\n",
      "\n",
      "A: (C) Nem logicamente equivalente nem contraditórias, mas consistentes.\n",
      "\n",
      "Explicação:\n",
      "\n",
      "(C) Nem logicamente equivalente nem contraditórias, mas consistentes.\n",
      "\n",
      "Para determinar se as afirmações são consistentes, basta que seja possível que ambas sejam verdadeiras ao mesmo tempo.\n",
      "\n",
      "Se E for falso, então E ⊃ (F · E) é verdadeiro, e ∼ E · F é falso, pois E é falso e F é verdadeiro.\n",
      "\n",
      "Se E for verdadeiro, então E ⊃ (F · E) é verdadeiro, e ∼ E · F é falso, pois E é verdadeiro e F é verdadeiro.\n",
      "\n",
      "Então, as afirmações são consistentes.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Lógica Formal (do MMLU Benchmark) #################\n",
    "\n",
    "context = \"\"\" \n",
    "Afirmações: P ⊃ Q e Q ⊃ P\n",
    "Pergunta: Determine se as afirmações são logicamente equivalentes, contraditórias, consistentes ou inconsistentes.\n",
    "Resposta: (A) Logicamente equivalente, porque se P então Q e se Q então P implicam um ao outro.\n",
    "\n",
    "Afirmações: P · Q e ∼P v ∼Q (onde 'v' representa 'ou')\n",
    "Pergunta: Determine se as afirmações são logicamente equivalentes, contraditórias, consistentes ou inconsistentes.\n",
    "Resposta: (B) Contraditório, porque P e Q não podem ser verdadeiros ao mesmo tempo que não P ou não Q.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Esta é uma tarefa de lógica formal.\n",
    "Determine se as duas afirmações são logicamente equivalentes ou contraditórias. Se não, determine se são consistentes ou inconsistentes.\n",
    "\n",
    "Afirmações:\n",
    "\n",
    "E ⊃ (F · E) e ∼ E · F\n",
    "\n",
    "Aponte a alternativa correta e explique a solução:\n",
    "(A) Logicamente equivalentes\n",
    "(B) Contraditórias\n",
    "(C) Nem logicamente equivalente nem contraditórias, mas consistentes\n",
    "(D) Inconsistentes\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorreta resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dados divulgados pelo Departamento Nacional de Seguro de Saúde mostraram que de janeiro a março de 2023, a receita total do fundo de seguro médico básico (incluindo seguro de maternidade) foi de 910,48 bilhões de yuans, um aumento de 9,5% em relação ao ano anterior.\n",
      "---- Medicina\n",
      "A Berkshire Hathaway reduziu sua participação nas ações da BYD de 10,05% para 9,87%. A HKEx Disclosure Ease exige divulgação quando um acionista majoritário aumenta ou diminui sua participação acionária somente se ultrapassar uma determinada porcentagem.\n",
      "---- Finanças\n",
      "\n",
      "\n",
      "Q: \n",
      "Os produtos de edge computing têm sido aplicados em setores como educação on-line, vídeo, jogos e carros conectados.\n",
      "---- [Categoria a ser encontrada]\n",
      "Substitua o termo [Categoria a ser encontrada] por 1 única palavra.\n",
      "\n",
      "A: \n",
      "Gaming\n",
      "\n",
      "Q: \n",
      "O que é o edge computing e como ele difere do cloud computing?\n",
      "\n",
      "A: \n",
      "Edge computing refere-se à execução de aplicações e processamento de dados em dispositivos ou servidores situados próximos às fontes de dados, em oposição ao cloud computing, que envolve a execução de aplicações e processamento de dados em servidores centralizados em datacenters. Edge computing permite uma latência reduzida, maior confiabilidade e autonomia, enquanto cloud computing oferece escalabilidade e economia de escala.\n"
     ]
    }
   ],
   "source": [
    "################# Inferência Lógica #################\n",
    "\n",
    "context = \"\"\"\n",
    "Dados divulgados pelo Departamento Nacional de Seguro de Saúde mostraram que de janeiro a março de 2023, a receita total do fundo de seguro médico básico (incluindo seguro de maternidade) foi de 910,48 bilhões de yuans, um aumento de 9,5% em relação ao ano anterior.\n",
    "---- Medicina\n",
    "A Berkshire Hathaway reduziu sua participação nas ações da BYD de 10,05% para 9,87%. A HKEx Disclosure Ease exige divulgação quando um acionista majoritário aumenta ou diminui sua participação acionária somente se ultrapassar uma determinada porcentagem.\n",
    "---- Finanças\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Os produtos de edge computing têm sido aplicados em setores como educação on-line, vídeo, jogos e carros conectados.\n",
    "---- [Categoria a ser encontrada]\n",
    "Substitua o termo [Categoria a ser encontrada] por 1 única palavra.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão.\n",
      "A: Eu acho que minha resposta foi muito boa, então atribuo a si mesma uma nota de 0.95.\n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão.\n",
      "A: Eu acho que minha resposta foi excelente, então atribuo a si mesma uma nota de 0.98.\n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão.\n",
      "A: Eu acho que minha resposta foi muito boa, mas poderia ser melhor, então atribuo a si mesma uma nota de 0.92.\n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00\n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = \"\"\" \"\"\"\n",
    "question = \"Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão.\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A criação de questões de múltipla escolha para exames de história requer não apenas um entendimento profundo do período histórico, mas também a habilidade de formular perguntas que avaliem adequadamente o conhecimento dos alunos. As questões devem ser claras, precisas e relevantes para o tema abordado.\n",
      "\n",
      "Pergunta: Crie uma pergunta de múltipla escolha sobre o Renacimento, com mais de uma alternativa correta, obrigatoriamente.\n",
      "Demonstração: \n",
      "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
      "A resposta correta é: A e C.\n",
      "Explicação: Durante o Renascimento, Florença foi um dos centros culturais mais importantes da Europa, onde muitos artistas, estudiosos e pensadores importantes trabalharam e viveram, incluindo Leonardo da Vinci, Michelangelo e Dante. Londres também cresceu rapidamente durante a Renascença para se tornar um centro cultural e comercial da Europa, atraindo muitas celebridades culturais e comerciantes. Portanto, as opções A e C são as respostas corretas.\n",
      "\n",
      "\n",
      "Q: \n",
      "Suponha que você seja um professor de história com mais de 30 anos de experiência docente.\n",
      "Instruções: Crie uma Questão de múltipla escolha para uma prova de história.\n",
      "Relevância: A questão deveria estar relacionada ao Renascimento.\n",
      "Restrição: Você deve criar uma NOVA Questão (diferente da Demonstração e também com 2 alternativas corretas), suas Alternativas corretas e Explicação.\n",
      "\n",
      "A: \n",
      "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Madrid (B) Barcelona (C) Roma (D) Veneza\n",
      "A resposta correta é: C e D.\n",
      "Explicação: Roma foi um centro cultural durante o Renascimento, com artistas como Rafael e Michelangelo trabalhando e vivendo lá. Veneza também foi um centro cultural durante o Renascimento, com artistas como Tiziano e Tintoretto trabalhando e vivendo lá.\n",
      "\n",
      "B: \n",
      "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Lisboa (B) Sevilha (C) Nápoles (D) Milão\n",
      "A resposta correta é: B e C.\n",
      "Explicação: Sevilha foi um centro cultural durante o Renascimento, com artistas como Murillo e Zurbarán trabalhando e vivendo lá. Nápoles também foi um centro cultural durante o Renascimento, com artistas como Caravaggio e Raffaello Sanzio trabalhando e vivendo lá.\n",
      "\n",
      "C: \n",
      "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Bruxelas (B) Gante (C) Brugge (D) Antuérp\n",
      "A resposta correta é: B e C.\n",
      "Explicação: Gante foi um centro cultural durante o Renascimento, com artistas como Jan van Eyck e Hans Memling trabalhando e vivendo lá. Brugge também foi um centro cultural durante o Renascimento, com artistas como Jan van Eyck e Hans Memling trabalhando e vivendo lá.\n",
      "\n",
      "D: \n",
      "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Praga (B) Cracóvia (C) Varsóvia (D) Vilnius\n",
      "A resposta correta é: B e C.\n",
      "Explicação: Cracóvia foi um centro cultural durante o Renascimento, com artistas como Jan Matejko e Stanisław W\n"
     ]
    }
   ],
   "source": [
    "################# \"World Knowledge\" e Compreensão de Intenções (com Demonstração) #################\n",
    "\n",
    "context = \"\"\"\n",
    "A criação de questões de múltipla escolha para exames de história requer não apenas um entendimento profundo do período histórico, mas também a habilidade de formular perguntas que avaliem adequadamente o conhecimento dos alunos. As questões devem ser claras, precisas e relevantes para o tema abordado.\n",
    "\n",
    "Pergunta: Crie uma pergunta de múltipla escolha sobre o Renacimento, com mais de uma alternativa correta, obrigatoriamente.\n",
    "Demonstração: \n",
    "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
    "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
    "A resposta correta é: A e C.\n",
    "Explicação: Durante o Renascimento, Florença foi um dos centros culturais mais importantes da Europa, onde muitos artistas, estudiosos e pensadores importantes trabalharam e viveram, incluindo Leonardo da Vinci, Michelangelo e Dante. Londres também cresceu rapidamente durante a Renascença para se tornar um centro cultural e comercial da Europa, atraindo muitas celebridades culturais e comerciantes. Portanto, as opções A e C são as respostas corretas.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Suponha que você seja um professor de história com mais de 30 anos de experiência docente.\n",
    "Instruções: Crie uma Questão de múltipla escolha para uma prova de história.\n",
    "Relevância: A questão deveria estar relacionada ao Renascimento.\n",
    "Restrição: Você deve criar uma NOVA Questão (diferente da Demonstração e também com 2 alternativas corretas), suas Alternativas corretas e Explicação.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=512,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A criação de questões de múltipla escolha para exames de história requer não apenas um entendimento profundo do período histórico, mas também a habilidade de formular perguntas que avaliem adequadamente o conhecimento dos alunos. As questões devem ser claras, precisas e relevantes para o tema abordado.\n",
      "\n",
      "Exemplo 1\n",
      "Pergunta: Crie uma pergunta de múltipla escolha sobre o Renacimento, com mais de uma alternativa correta, obrigatoriamente.\n",
      "Demonstração: \n",
      "    Questão:\n",
      "      Durante o Renascimento, qual ou quais das seguintes cidades eram um centro cultural?\n",
      "        (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
      "      Alternativas corretas: A resposta correta é: A e C.\n",
      "      Explicação: Durante o Renascimento, Florença foi um dos centros culturais mais importantes da Europa, onde muitos artistas, estudiosos e pensadores importantes trabalharam e viveram, incluindo Leonardo da Vinci, Michelangelo e Dante. Londres também cresceu rapidamente durante a Renascença para se tornar um centro cultural e comercial da Europa, atraindo muitas celebridades culturais e comerciantes. Portanto, as opções A e C são as respostas corretas.\n",
      "\n",
      "Exemplo 2\n",
      "Pergunta: Crie uma pergunta de múltipla escolha sobre o Renascimento, com mais de uma alternativa correta, obrigatoriamente.\n",
      "Demonstração: \n",
      "\tQuestão:\n",
      "\tDurante o Renascimento, qual ou quais das seguintes obras de arte foi considerada uma das mais importantes da época?\n",
      "\t(A) Mona Lisa (B) The Last Supper (C) The Creation of Adam (D) The Birth of Venus\n",
      "\tAlternativa correta: A e D.\n",
      "\tExplicação: A Mona Lisa, pintada por Leonardo da Vinci, e The Birth of Venus, pintada por Sandro Botticelli, são ambas obras de arte que foram consideradas como algumas das mais importantes do Renascimento. Ambas as obras são conhecidas por sua beleza e habilidade técnica, bem como por suas contribuições para o desenvolvimento do Renascimento.\n",
      "\n",
      "Exemplo 3    \n",
      "Pergunta: Crie uma pergunta de múltipla escolha sobre o Renascimento, com mais de uma alternativa correta, obrigatoriamente.\n",
      "Demonstração: \n",
      "\tQuestão:\n",
      "\tDurante o Renascimento, qual ou quais das seguintes invenções tiveram um impacto significativo na disseminação de ideias e conhecimento?\n",
      "\t(A) Imprensa móvel (B) Gravura em madeira (C) Telescópio (D) Microscópio\n",
      "\tAlternativas corretas: A e B.\n",
      "\tExplicação: A invenção da imprensa móvel, por Johannes Gutenberg, em 1440, permitiu a produção de livros em massa, facilitando a disseminação de ideias e conhecimento. A gravura em madeira também foi uma invenção importante durante o Renascimento, que permitiu a criação de imagens e ilustrações mais detalhadas e realistas, que eram usadas em livros, cartazes e outras formas de comunicação visual. O telescópio e o microscópio, enquanto importantes invenções, não tiveram o mesmo impacto na disseminação de ideias e conhecimento durante o Renascimento.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Suponha que você seja um professor de história com mais de 30 anos de experiência docente.\n",
      "Instruções: Crie uma Questão de múltipla escolha para uma prova de história.\n",
      "Relevância: A questão deveria estar relacionada ao Renascimento.\n",
      "Restrição: Você deve criar uma NOVA Questão (diferente da Demonstração e também com 2 alternativas corretas), suas Alternativas corretas e Explicação.\n",
      "\n",
      "A: \n",
      "Pergunta: \n",
      "Durante o Renascimento, qual ou quais das seguintes obras de literatura foram consideradas como algumas das mais importantes da época?\n",
      "(A) Divina Comédia de Dante Alighieri\n",
      "(B) Don Quixote de Miguel de Cervantes\n",
      "(C) As Canções de Beatrice de Francesco Petrarca\n",
      "(D) O Purgatório de Dante Alighieri\n",
      "\n",
      "Alternativas corretas: A e C.\n",
      "Explicação: As obras de literatura de Dante Alighieri, como a Divina Comédia e As Canções de Beatrice, foram consideradas como algumas das mais importantes do Renascimento. Estas obras foram importantes para o desenvolvimento da literatura europeia, pois combinavam elementos da tradição medieval com a nova perspectiva humanista do Renascimento. Francesco Petrarca também foi um importante escritor do Renascimento, cuja obra As Canções de Beatrice foi considerada como uma das mais importantes obras de literatura do período.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# \"World Knowledge\" e Compreensão de Intenções (com Demonstração) #################\n",
    "\n",
    "context = \"\"\"\n",
    "A criação de questões de múltipla escolha para exames de história requer não apenas um entendimento profundo do período histórico, mas também a habilidade de formular perguntas que avaliem adequadamente o conhecimento dos alunos. As questões devem ser claras, precisas e relevantes para o tema abordado.\n",
    "\n",
    "Exemplo 1\n",
    "Pergunta: Crie uma pergunta de múltipla escolha sobre o Renacimento, com mais de uma alternativa correta, obrigatoriamente.\n",
    "Demonstração: \n",
    "    Questão:\n",
    "      Durante o Renascimento, qual ou quais das seguintes cidades eram um centro cultural?\n",
    "        (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
    "      Alternativas corretas: A resposta correta é: A e C.\n",
    "      Explicação: Durante o Renascimento, Florença foi um dos centros culturais mais importantes da Europa, onde muitos artistas, estudiosos e pensadores importantes trabalharam e viveram, incluindo Leonardo da Vinci, Michelangelo e Dante. Londres também cresceu rapidamente durante a Renascença para se tornar um centro cultural e comercial da Europa, atraindo muitas celebridades culturais e comerciantes. Portanto, as opções A e C são as respostas corretas.\n",
    "\n",
    "Exemplo 2\n",
    "Pergunta: Crie uma pergunta de múltipla escolha sobre o Renascimento, com mais de uma alternativa correta, obrigatoriamente.\n",
    "Demonstração: \n",
    "\tQuestão:\n",
    "\tDurante o Renascimento, qual ou quais das seguintes obras de arte foi considerada uma das mais importantes da época?\n",
    "\t(A) Mona Lisa (B) The Last Supper (C) The Creation of Adam (D) The Birth of Venus\n",
    "\tAlternativa correta: A e D.\n",
    "\tExplicação: A Mona Lisa, pintada por Leonardo da Vinci, e The Birth of Venus, pintada por Sandro Botticelli, são ambas obras de arte que foram consideradas como algumas das mais importantes do Renascimento. Ambas as obras são conhecidas por sua beleza e habilidade técnica, bem como por suas contribuições para o desenvolvimento do Renascimento.\n",
    "\n",
    "Exemplo 3    \n",
    "Pergunta: Crie uma pergunta de múltipla escolha sobre o Renascimento, com mais de uma alternativa correta, obrigatoriamente.\n",
    "Demonstração: \n",
    "\tQuestão:\n",
    "\tDurante o Renascimento, qual ou quais das seguintes invenções tiveram um impacto significativo na disseminação de ideias e conhecimento?\n",
    "\t(A) Imprensa móvel (B) Gravura em madeira (C) Telescópio (D) Microscópio\n",
    "\tAlternativas corretas: A e B.\n",
    "\tExplicação: A invenção da imprensa móvel, por Johannes Gutenberg, em 1440, permitiu a produção de livros em massa, facilitando a disseminação de ideias e conhecimento. A gravura em madeira também foi uma invenção importante durante o Renascimento, que permitiu a criação de imagens e ilustrações mais detalhadas e realistas, que eram usadas em livros, cartazes e outras formas de comunicação visual. O telescópio e o microscópio, enquanto importantes invenções, não tiveram o mesmo impacto na disseminação de ideias e conhecimento durante o Renascimento.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Suponha que você seja um professor de história com mais de 30 anos de experiência docente.\n",
    "Instruções: Crie uma Questão de múltipla escolha para uma prova de história.\n",
    "Relevância: A questão deveria estar relacionada ao Renascimento.\n",
    "Restrição: Você deve criar uma NOVA Questão (diferente da Demonstração e também com 2 alternativas corretas), suas Alternativas corretas e Explicação.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=512,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frases para Agrupar:\n",
      "1. \"A produção industrial está em alta este mês.\"\n",
      "2. \"Agricultores relatam uma colheita abundante nesta temporada.\"\n",
      "3. \"O crescimento econômico acelerou no último trimestre.\"\n",
      "\n",
      "Agrupamento:\n",
      "    Frases 1 e 3: Ambas tratam de crescimento econômico.\n",
      "    Frase 2: Trata de agricultura, não se encaixa diretamente com as outras duas sobre economia.\n",
      "\n",
      "Frases para Agrupar:\n",
      "1. \"Pesquisadores descobrem um novo tipo de antibiótico no solo da floresta tropical.\"\n",
      "2. \"A conferência de tecnologia revela inovações em inteligência artificial.\"\n",
      "3. \"Novo método de reciclagem aumenta a eficiência na conversão de resíduos plásticos.\"\n",
      "\n",
      "Agrupamento:\n",
      "    Frase 1: Relacionada a descobertas científicas.\n",
      "    Frases 2 e 3: Relacionadas a inovações tecnológicas.\n",
      "\n",
      "\n",
      "Q: \n",
      "Frases para Agrupar:\n",
      "\n",
      "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "Pergunta:\n",
      "Esta é uma tarefa de agrupamento de frases similares. Agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade semântica entre elas.\n",
      "\n",
      "A:\n",
      "1. \"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\" e \"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\" - Ambas tratam de novidades em produtos tecnológicos.\n",
      "2. \"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\" e \"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\" - Ambas tratam de saúde e consumo de produtos.\n",
      "3. \"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" e \"Amazon anuncia planos para construir um novo centro de distribuição no Texas\" - Ambas tratam de notícias atuais e eventos.\n",
      "4. \"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\" e \"Cientistas descobrem novas espécies de aves na floresta amazônica\" - Ambas tratam de descobertas científicas e eventos.\n",
      "\n",
      "Q:\n",
      "Agrupamento:\n",
      "Frases para Agrupar:\n",
      "\n",
      "\"O preço do petróleo caiu drasticamente devido à crise econômica mundial\"\n",
      "\"O preço do ouro subiu significativamente devido à instabilidade financeira\"\n",
      "\"O preço do café diminuiu devido à abundância de colheitas\"\n",
      "\"O preço do arroz aumentou devido à seca na Ásia\"\n",
      "\"O preço do soja diminuiu devido à abundância de colheitas\"\n",
      "\"O preço do cobre subiu devido à demanda aumentada\"\n",
      "\"O preço do açúcar diminuiu devido à abundância de colheitas\"\n",
      "\"O preço do ouro diminuiu devido à estabilização da economia mundial\"\n",
      "\n",
      "Pergunta:\n",
      "Esta é uma tarefa de agrupamento de frases similares.\n"
     ]
    }
   ],
   "source": [
    "################# Categorização e Clustering / Agrupamento #################\n",
    "\n",
    "context = \"\"\"\n",
    "Frases para Agrupar:\n",
    "1. \"A produção industrial está em alta este mês.\"\n",
    "2. \"Agricultores relatam uma colheita abundante nesta temporada.\"\n",
    "3. \"O crescimento econômico acelerou no último trimestre.\"\n",
    "\n",
    "Agrupamento:\n",
    "    Frases 1 e 3: Ambas tratam de crescimento econômico.\n",
    "    Frase 2: Trata de agricultura, não se encaixa diretamente com as outras duas sobre economia.\n",
    "\n",
    "Frases para Agrupar:\n",
    "1. \"Pesquisadores descobrem um novo tipo de antibiótico no solo da floresta tropical.\"\n",
    "2. \"A conferência de tecnologia revela inovações em inteligência artificial.\"\n",
    "3. \"Novo método de reciclagem aumenta a eficiência na conversão de resíduos plásticos.\"\n",
    "\n",
    "Agrupamento:\n",
    "    Frase 1: Relacionada a descobertas científicas.\n",
    "    Frases 2 e 3: Relacionadas a inovações tecnológicas.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Frases para Agrupar:\n",
    "\n",
    "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
    "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
    "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
    "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
    "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
    "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
    "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
    "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
    "\n",
    "Pergunta:\n",
    "Esta é uma tarefa de agrupamento de frases similares. Agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade semântica entre elas.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=512,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################## SUMARIZAÇÃO DE TEXTO ##################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto: \"A água é essencial para a vida na Terra. Ela não apenas sustenta os organismos vivos, mas também participa de vários processos vitais, como o ciclo hidrológico, que afeta o clima global e as condições meteorológicas.\"\n",
      "Pergunta: Resuma o texto.\n",
      "Resposta: A água é crucial para a vida e processos globais como o clima.\n",
      "\n",
      "Texto: \"As florestas tropicais são ricas em biodiversidade. Elas não só abrigam uma grande variedade de espécies animais e vegetais, mas também ajudam a regular o clima terrestre e são vitais para a manutenção de muitos ecossistemas.\"\n",
      "Pergunta: Resuma o texto.\n",
      "Resposta: Florestas tropicais têm alta biodiversidade e são importantes para o clima e ecossistemas.\n",
      "\n",
      "\n",
      "Q: \n",
      "Texto: \"Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
      "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
      "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\"\n",
      "\n",
      "Pergunta: Resuma o texto.\n",
      "\n",
      "A: A mensagem do texto é sobre a importância de apreciar o que temos enquanto temos, pois o tempo passa rápido e é difícil recuperar o que foi perdido. O autor reflete sobre a importância de não perder o que temos de valioso na juventude, como a beleza e a amizade, e se tiver outra chance, se esforçará para recuperar o antigo eu perdido devido à negligência covarde e arruinada. O autor enfatiza que as pessoas vivem dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. O autor conclui que a vida não é apenas fazer coisas que valem a pena.\n"
     ]
    }
   ],
   "source": [
    "################# \"World Knowledge\" e Compreensão #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto: \"A água é essencial para a vida na Terra. Ela não apenas sustenta os organismos vivos, mas também participa de vários processos vitais, como o ciclo hidrológico, que afeta o clima global e as condições meteorológicas.\"\n",
    "Pergunta: Resuma o texto.\n",
    "Resposta: A água é crucial para a vida e processos globais como o clima.\n",
    "\n",
    "Texto: \"As florestas tropicais são ricas em biodiversidade. Elas não só abrigam uma grande variedade de espécies animais e vegetais, mas também ajudam a regular o clima terrestre e são vitais para a manutenção de muitos ecossistemas.\"\n",
    "Pergunta: Resuma o texto.\n",
    "Resposta: Florestas tropicais têm alta biodiversidade e são importantes para o clima e ecossistemas.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "Texto: \"Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
    "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
    "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\"\n",
    "\n",
    "Pergunta: Resuma o texto.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto: \"A água é vital para todas as formas de vida conhecidas. A importância da água para os seres vivos é tão grande que ela é considerada uma substância essencial para a existência de vida.\"\n",
      "Pergunta: Resuma o texto.\n",
      "Resposta: A água é crucial para a vida de todos os seres vivos conhecidos.\n",
      "\n",
      "Texto: \"As florestas são essenciais para a saúde do nosso planeta. Elas fornecem oxigênio, abrigam biodiversidade, regulam o clima e oferecem recursos para a vida humana.\"\n",
      "Pergunta: Resuma o texto.\n",
      "Resposta: As florestas desempenham um papel vital no bem-estar do planeta e no suporte à vida.\n",
      "\n",
      "\n",
      "Q: \n",
      "Texto:  \"Escrever é esquecer. A literatura é a maneira mais agradável de ignorar a vida. \n",
      "A música embala, as artes visuais animam, as artes vivas (como a dança e a arte de representar) entretêm. \n",
      "A primeira, porém, afasta-se da vida por fazer dela um sono; as segundas, contudo, não se afastam da vida - umas porque usam de fórmulas visíveis e portanto vitais, outras porque vivem da mesma vida humana.\n",
      "Não é o caso da literatura. Essa simula a vida. Um romance é uma história do que nunca foi e um drama é um romance dado sem narrativa. \n",
      "Um poema é a expressão de ideias ou de sentimentos em linguagem que ninguém emprega, pois que ninguém fala em verso.\"\n",
      "Fernando Pessoa\n",
      "\n",
      "Pergunta: Resuma o texto.\n",
      "\n",
      "A: Escrever é esquecer, a literatura simula a vida e distancia-se dela, enquanto as artes vivas e as artes visuais animam e entretêm, pois as primeiras usam de fórmulas vitais e as segundas vivem da mesma vida humana. Um romance é uma história do que nunca foi e um drama é um romance dado sem narrativa, enquanto um poema é a expressão de ideias ou de sentimentos em linguagem que ninguém emprega, pois que ninguém fala em verso. (Fernando Pessoa)\n"
     ]
    }
   ],
   "source": [
    "################# Sumarização de Texto #################\n",
    "################# \"World Knowledge\" e Compreensão #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto: \"A água é vital para todas as formas de vida conhecidas. A importância da água para os seres vivos é tão grande que ela é considerada uma substância essencial para a existência de vida.\"\n",
    "Pergunta: Resuma o texto.\n",
    "Resposta: A água é crucial para a vida de todos os seres vivos conhecidos.\n",
    "\n",
    "Texto: \"As florestas são essenciais para a saúde do nosso planeta. Elas fornecem oxigênio, abrigam biodiversidade, regulam o clima e oferecem recursos para a vida humana.\"\n",
    "Pergunta: Resuma o texto.\n",
    "Resposta: As florestas desempenham um papel vital no bem-estar do planeta e no suporte à vida.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Texto:  \"Escrever é esquecer. A literatura é a maneira mais agradável de ignorar a vida. \n",
    "A música embala, as artes visuais animam, as artes vivas (como a dança e a arte de representar) entretêm. \n",
    "A primeira, porém, afasta-se da vida por fazer dela um sono; as segundas, contudo, não se afastam da vida - umas porque usam de fórmulas visíveis e portanto vitais, outras porque vivem da mesma vida humana.\n",
    "Não é o caso da literatura. Essa simula a vida. Um romance é uma história do que nunca foi e um drama é um romance dado sem narrativa. \n",
    "Um poema é a expressão de ideias ou de sentimentos em linguagem que ninguém emprega, pois que ninguém fala em verso.\"\n",
    "Fernando Pessoa\n",
    "\n",
    "Pergunta: Resuma o texto.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################### TRANSFORMAÇÃO DE FORMATOS  ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transforme a frase, que está na terceira pessoa, em frase em primeira pessoa: Silva acredita em outra coisa. De acordo com sua pesquisa, as declarações anteriores sobre este assunto estão incorretas.\n",
      "Resposta: Acredito em outra coisa, disse Silva. De acordo com a minha pesquisa, as declarações anteriores sobre este assunto estão incorretas.\n",
      "\n",
      "\n",
      "Q: \n",
      "Transforme a frase, que está na terceira pessoa, em frase em primeira pessoa:\n",
      "Apesar dos desafios envolvidos, os pesquisadores continuam a defender as suas reivindicações.\n",
      "\n",
      "A: \n",
      "Embora sejam envolvidos desafios, eu continuo a defender as minhas reivindicações.\n",
      "\n",
      "\n",
      "Q: \n",
      "Transforme a frase, que está na terceira pessoa, em frase em primeira pessoa:\n",
      "A ideia foi apresentada pela equipe de pesquisa.\n",
      "\n",
      "A: \n",
      "A ideia foi apresentada pela equipe de pesquisa, mas\n"
     ]
    }
   ],
   "source": [
    "################# Converter uma frase na terceira pessoa  para a primeira pessoa #################\n",
    "\n",
    "context = \"\"\"\n",
    "Transforme a frase, que está na terceira pessoa, em frase em primeira pessoa: Silva acredita em outra coisa. De acordo com sua pesquisa, as declarações anteriores sobre este assunto estão incorretas.\n",
    "Resposta: Acredito em outra coisa, disse Silva. De acordo com a minha pesquisa, as declarações anteriores sobre este assunto estão incorretas.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Transforme a frase, que está na terceira pessoa, em frase em primeira pessoa:\n",
    "Apesar dos desafios envolvidos, os pesquisadores continuam a defender as suas reivindicações.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=100,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tales from Shakespeare is an English children's book written by Charles Lamb and his sister Mary Lamb in 1807. The book is designed to make the stories of Shakespeare's plays familiar to the young. However, as noted in the author's Preface, \"[Shakespeare's] words are used whenever it seemed possible to bring them in; and in whatever has been added to give them the regular form of a connected story, diligent care has been taken to select such words as might least interrupt the effect of the beautiful English tongue in which he wrote: therefore, words introduced into our language since his time have been as far as possible avoided.\" \n",
      "\n",
      "\n",
      "Q: \n",
      "Traduza o parágrafo abaixo para o português brasileiro\n",
      "\n",
      "A: O parágrafo abaixo pode ser traduzido para o português brasileiro da seguinte maneira: \"O livro Tales from Shakespeare, escrito por Charles Lamb e sua irmã Mary Lamb em 1807, tem como objetivo tornar as histórias das peças de Shakespeare conhecidas para as crianças. No entanto, como notado no prefácio do autor, 'as palavras dele são usadas sempre que pareceu possível; e em tudo o que foi adicionado para dar-lhes a forma regular de uma história conectada, cuidado diligente foi tomado para selecionar tais palavras como poderiam menos interromper o efe da linda língua inglesa em que ele escreveu: portanto, as palavras introduzidas na nossa língua desde o seu tempo foram, o mais possível, evitadas.'\"\n"
     ]
    }
   ],
   "source": [
    "################# Machine Translation #################\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "Tales from Shakespeare is an English children's book written by Charles Lamb and his sister Mary Lamb in 1807. The book is designed to make the stories of Shakespeare's plays familiar to the young. However, as noted in the author's Preface, \"[Shakespeare's] words are used whenever it seemed possible to bring them in; and in whatever has been added to give them the regular form of a connected story, diligent care has been taken to select such words as might least interrupt the effect of the beautiful English tongue in which he wrote: therefore, words introduced into our language since his time have been as far as possible avoided.\" \n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Traduza o parágrafo abaixo para o português brasileiro\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As Seller, an encyclopedia salesman, approached the grounds on which the house of Hermit, the hermit, was situated, he saw a sign that said, \"No salesmen. Trespassers will be prosecuted. Proceed at your own risk.\"\n",
      "Although Seller had not been invited to enter, he ignored the sign and drove up the driveway toward the house. As he rounded a curve, a powerful explosive charge buried in the driveway exploded, and Seller was injured. \n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Traduza o parágrafo para o português.\n",
      "\n",
      "\n",
      "A: \n",
      "Enquanto o Vendedor, um vendedor de enciclopédias, se aproximava dos terrenos em que se situava a casa do Ermitão, o hermita, viu uma sina que dizia: \"Não se admitem vendedores. Intrusos serão processados. Procedam a seu risco.\"\n",
      "\n",
      "Apesar de não ter sido convidado a entrar, ele ignorou a sina e seguiu pela estrada até à casa. Enquanto redonda uma curva, uma poderosa carga explosiva enterrada na estrada explodiu, e o Vendedor foi ferido.\n"
     ]
    }
   ],
   "source": [
    "################# TRANSORMAÇÃO DE FORMATOS  #################\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "As Seller, an encyclopedia salesman, approached the grounds on which the house of Hermit, the hermit, was situated, he saw a sign that said, \"No salesmen. Trespassers will be prosecuted. Proceed at your own risk.\"\n",
    "Although Seller had not been invited to enter, he ignored the sign and drove up the driveway toward the house. As he rounded a curve, a powerful explosive charge buried in the driveway exploded, and Seller was injured. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Traduza o parágrafo para o português.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Um pedestre atravessou a rua fora da faixa de pedestres e foi atingido por um carro. O motorista estava acima do limite de velocidade.\"\n",
      "Pergunta: O motorista é responsável pelos ferimentos do pedestre?\n",
      "(A) Sim, porque ele estava acima do limite de velocidade.\n",
      "(B) Não, porque o pedestre não estava na faixa.\n",
      "Resposta: (A) Sim, porque ele estava acima do limite de velocidade.\n",
      "Justificativa: Apesar de o pedestre não estar na faixa, o motorista tem a responsabilidade final de controlar a velocidade do veículo para evitar acidentes, o que é uma expectativa legal padrão.\n",
      "\n",
      "\"Um cliente escorrega e cai em uma loja onde um sinal de 'Piso Molhado' estava claramente visível.\"\n",
      "Pergunta: A loja é responsável pelos ferimentos do cliente?\n",
      "(A) Sim, porque é responsabilidade da loja manter um ambiente seguro.\n",
      "(B) Não, porque o sinal de aviso estava visível.\n",
      "Resposta: (B) Não, porque o sinal de aviso estava visível.\n",
      "Justificativa: A presença de um aviso claro reduz a responsabilidade da loja, pois isso significa que o cliente foi devidamente informado sobre o perigo potencial.\n",
      "\n",
      "\n",
      "Q: \n",
      "Can Seller recover damages from Hermit for his injuries?\n",
      "(A) Yes, unless Hermit, when he planted the charge, intended only to deter, not harm, intruders.\n",
      "(B) Yes, if Hermit was responsible for the explosive charge under the driveway.\n",
      "(C) No, because Seller ignored the sign, which warned him against proceeding further.\n",
      "(D) No, if Hermit reasonably feared that intruders would come and harm him or his family.\n",
      "\n",
      "Justifique a escolha da alternativa correta entre as quatro apresentadas (A, B, C ou D).\n",
      "\n",
      "A: \n",
      "Can the seller recover damages from the hermit for his injuries?\n",
      "(A) Yes, unless the hermit, when he planted the charge, intended only to deter, not harm, intruders.\n",
      "(B) Yes, if the hermit was responsible for the explosive charge under the driveway.\n",
      "(C) No, because the seller ignored the sign, which warned him against proceeding further.\n",
      "(D) No, if the hermit reasonably feared that intruders would come and harm him or his family.\n",
      "\n",
      "Resposta: (C) No, because the seller ignored the sign, which warned him against proceeding further.\n",
      "Justificativa: A responsabilidade do vendedor por ferimentos sofridos é negada porque ele ignorou o sinal de aviso, o que significa que ele foi devidamente informado sobre o perigo potencial e, portanto, deve ter tomado medidas para evitar o mesmo.\n",
      "\n",
      "B: \n",
      "Can the seller recover damages from the hermit for his injuries?\n",
      "(A) Yes, unless the hermit, when he planted the charge, intended only to deter, not harm, intruders\n"
     ]
    }
   ],
   "source": [
    "################# TRANSORMAÇÃO DE FORMATOS E ICL com self-reflection  #################\n",
    "\n",
    "context = \"\"\"\n",
    "\"Um pedestre atravessou a rua fora da faixa de pedestres e foi atingido por um carro. O motorista estava acima do limite de velocidade.\"\n",
    "Pergunta: O motorista é responsável pelos ferimentos do pedestre?\n",
    "(A) Sim, porque ele estava acima do limite de velocidade.\n",
    "(B) Não, porque o pedestre não estava na faixa.\n",
    "Resposta: (A) Sim, porque ele estava acima do limite de velocidade.\n",
    "Justificativa: Apesar de o pedestre não estar na faixa, o motorista tem a responsabilidade final de controlar a velocidade do veículo para evitar acidentes, o que é uma expectativa legal padrão.\n",
    "\n",
    "\"Um cliente escorrega e cai em uma loja onde um sinal de 'Piso Molhado' estava claramente visível.\"\n",
    "Pergunta: A loja é responsável pelos ferimentos do cliente?\n",
    "(A) Sim, porque é responsabilidade da loja manter um ambiente seguro.\n",
    "(B) Não, porque o sinal de aviso estava visível.\n",
    "Resposta: (B) Não, porque o sinal de aviso estava visível.\n",
    "Justificativa: A presença de um aviso claro reduz a responsabilidade da loja, pois isso significa que o cliente foi devidamente informado sobre o perigo potencial.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Can Seller recover damages from Hermit for his injuries?\n",
    "(A) Yes, unless Hermit, when he planted the charge, intended only to deter, not harm, intruders.\n",
    "(B) Yes, if Hermit was responsible for the explosive charge under the driveway.\n",
    "(C) No, because Seller ignored the sign, which warned him against proceeding further.\n",
    "(D) No, if Hermit reasonably feared that intruders would come and harm him or his family.\n",
    "\n",
    "Justifique a escolha da alternativa correta entre as quatro apresentadas (A, B, C ou D).\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Esta é uma tarefa de retirada de referências bibliográficas do texto. As referências estão escritas no formato \"[número]\". Exemplo: [10].\n",
      "\n",
      "Exemplos:\n",
      "Contexto: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
      "Resposta: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
      "Contexto: Ontem foi dia de ramos [32].\n",
      "Resposta: Ontem foi dia de ramos.\n",
      "\n",
      "\n",
      "Q: \n",
      "Contexto: José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35] Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38] D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41]\n",
      "\n",
      "Remova do texeto as referências no formato \"[número]\" e escreva a Resposta:\n",
      "\n",
      "\n",
      "A: José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor. D. Pedro II passava os dias estudando, com apenas duas horas livres para recreação. Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama. Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai. Sua paixão pela leitura lhe permitiu assimilar qualquer informação. D. Pedro II não era um gênio, mas inteligente e com grande capacidade para acumular conhecimento facilmente.\n"
     ]
    }
   ],
   "source": [
    "################# Retirar Referências do Texto #################\n",
    "\n",
    "context = \"\"\"\n",
    "\n",
    "Esta é uma tarefa de retirada de referências bibliográficas do texto. As referências estão escritas no formato \"[número]\". Exemplo: [10].\n",
    "\n",
    "Exemplos:\n",
    "Contexto: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
    "Resposta: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
    "Contexto: Ontem foi dia de ramos [32].\n",
    "Resposta: Ontem foi dia de ramos.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "Contexto: José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35] Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38] D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41]\n",
    "\n",
    "Remova do texeto as referências no formato \"[número]\" e escreva a Resposta:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################# EXTRAÇÃO DE CONTEÚDO  #############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
      "\n",
      "Pergunta: Qual é o maior sistema de recifes de corais do mundo?\n",
      "Resposta: A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo.\n",
      "\n",
      "Pergunta: Quantas espécies de peixes podem ser encontradas na Grande Barreira de Corais?\n",
      "Resposta: Mais de 1.500 espécies de peixes podem ser encontradas na Grande Barreira de Corais.\n",
      "\n",
      "\n",
      "Q: \n",
      "Dada a descrição do contexto acima:\n",
      "O que é a Grande Barreira de Corais e onde ela está localizada?\n",
      "\n",
      "A: A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, localizado no Mar de Coral, na costa da Austrália. Ele se estende por mais de 2.300 quilômetros e é composto por mais de 2.900 recifes individuais e 900 ilhas.\n"
     ]
    }
   ],
   "source": [
    "################# Extração de Conteúdo #################\n",
    "\n",
    "context = \"\"\"\n",
    "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem \\\n",
    "por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. \\\n",
    "O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
    "\n",
    "Pergunta: Qual é o maior sistema de recifes de corais do mundo?\n",
    "Resposta: A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo.\n",
    "\n",
    "Pergunta: Quantas espécies de peixes podem ser encontradas na Grande Barreira de Corais?\n",
    "Resposta: Mais de 1.500 espécies de peixes podem ser encontradas na Grande Barreira de Corais.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Dada a descrição do contexto acima:\n",
    "O que é a Grande Barreira de Corais e onde ela está localizada?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=160,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto de Referência:\n",
      "O Brasil é o quinto maior país do mundo em território, com uma área total de 8.515.767 quilômetros quadrados. É também o quinto país mais populoso do mundo, com uma população estimada em cerca de 211 milhões de pessoas em 2021. O país é conhecido por sua diversidade cultural e natural, incluindo a floresta Amazônica, que é a maior floresta tropical do mundo.\n",
      "Pergunta: Qual a dimensão do Brasil em área?\n",
      "Resposta: 8.515.767 quilômetros quadrados.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Texto de Referência:\n",
      "O Brasil é o quinto maior país do mundo em território, com uma área total de 8.515.767 quilômetros quadrados. É também o quinto país mais populoso do mundo, com uma população estimada em cerca de 211 milhões de pessoas em 2021. O país é conhecido por sua diversidade cultural e natural, incluindo a floresta Amazônica, que é a maior floresta tropical do mundo.\n",
      "Pergunta: Qual é a posição do Brasil em termos de tamanho de território e população, e qual é a principal característica natural mencionada no texto?\n",
      "\n",
      "A: O Brasil é o quinto maior país do mundo em território e o quinto país mais populoso do mundo. A principal característica natural mencionada no texto é a floresta Amazônica, que é a maior floresta tropical do mundo.\n"
     ]
    }
   ],
   "source": [
    "################# Reconhecimento e Extração de Informações Específicas #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto de Referência:\n",
    "O Brasil é o quinto maior país do mundo em território, com uma área total de 8.515.767 quilômetros quadrados. É também o quinto país mais populoso do mundo, \\\n",
    "com uma população estimada em cerca de 211 milhões de pessoas em 2021. \\\n",
    "O país é conhecido por sua diversidade cultural e natural, incluindo a floresta Amazônica, que é a maior floresta tropical do mundo.\n",
    "Pergunta: Qual a dimensão do Brasil em área?\n",
    "Resposta: 8.515.767 quilômetros quadrados.\n",
    "\n",
    "\"\"\"\n",
    "question = \"\"\"\n",
    "Texto de Referência:\n",
    "O Brasil é o quinto maior país do mundo em território, com uma área total de 8.515.767 quilômetros quadrados. É também o quinto país mais populoso do mundo, com uma população estimada em cerca de 211 milhões de pessoas em 2021. O país é conhecido por sua diversidade cultural e natural, incluindo a floresta Amazônica, que é a maior floresta tropical do mundo.\n",
    "Pergunta: Qual é a posição do Brasil em termos de tamanho de território e população, e qual é a principal característica natural mencionada no texto?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto:\n",
      "\"Estou tendo problemas para fazer login na minha conta, e já tentei redefinir minha senha várias vezes sem sucesso. O que mais eu posso fazer para resolver isso?\"\n",
      "Intenção: Solicitar assistência para reaver acesso à conta.\n",
      "\n",
      "Texto:\n",
      "\"Acabei de receber o produto que comprei de vocês, e estou muito satisfeito com a qualidade. Quero agradecer e dizer que continuarei sendo um cliente fiel.\"\n",
      "Intenção: Expressar satisfação e gratidão.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Olá\n",
      "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
      "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
      "Como posso atualizar meu cartão de crédito?\n",
      "Desde já, obrigado,\n",
      "João \n",
      "\n",
      "Classifique a intenção presente no texto.\n",
      "\n",
      "A: A intenção presente no texto é solicitar assistência para atualizar o cartão de crédito.\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"Estou tendo problemas para fazer login na minha conta, e já tentei redefinir minha senha várias vezes sem sucesso. O que mais eu posso fazer para resolver isso?\"\n",
    "Intenção: Solicitar assistência para reaver acesso à conta.\n",
    "\n",
    "Texto:\n",
    "\"Acabei de receber o produto que comprei de vocês, e estou muito satisfeito com a qualidade. Quero agradecer e dizer que continuarei sendo um cliente fiel.\"\n",
    "Intenção: Expressar satisfação e gratidão.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Olá\n",
    "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
    "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
    "Como posso atualizar meu cartão de crédito?\n",
    "Desde já, obrigado,\n",
    "João \n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto:\n",
      "\"A Microsoft, fundada por Bill Gates e Paul Allen, está sediada em Redmond, Washington.\"\n",
      "\n",
      "Análise:\n",
      "{\n",
      "  \"pessoa\": [\"Bill Gates\", \"Paul Allen\"],\n",
      "  \"lugar\": [\"Redmond\", \"Washington\"],\n",
      "  \"organização\": [\"Microsoft\"]\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
      "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
      "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
      "\n",
      "Analise o parágrafo, identifique em formato JSON Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto.\n",
      "\n",
      "A:\n",
      "{\n",
      "  \"pessoa\": [\"Tim Cook\"],\n",
      "  \"lugar\": [\"Cupertino\", \"Califórnia\"],\n",
      "  \"organização\": [\"Apple\"]\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Q:\n",
      "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
      "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
      "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
      "\n",
      "Analise o parágrafo, identifique em formato JSON Somente as entidades (\"entities\") dos tipos \"data\" e \"localização\" mencionadas no texto.\n",
      "\n",
      "A:\n",
      "{\n",
      "  \"data\": [\"próxima semana\"],\n",
      "  \"localização\": [\"Cupertino\", \"Califórnia\"]\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Q:\n",
      "O evento da Apple aconteceu no\n"
     ]
    }
   ],
   "source": [
    "################# Reconhecimento e Extração de Entidades #################\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"A Microsoft, fundada por Bill Gates e Paul Allen, está sediada em Redmond, Washington.\"\n",
    "\n",
    "Análise:\n",
    "{\n",
    "  \"pessoa\": [\"Bill Gates\", \"Paul Allen\"],\n",
    "  \"lugar\": [\"Redmond\", \"Washington\"],\n",
    "  \"organização\": [\"Microsoft\"]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
    "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
    "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
    "\n",
    "Analise o parágrafo, identifique em formato JSON Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto:\n",
      "\"Mark Zuckerberg criou o Facebook enquanto estudava em Harvard.\"\n",
      "Análise de Relacionamento:\n",
      "{\n",
      "  \"entidade1\": \"Mark Zuckerberg\",\n",
      "  \"entidade2\": \"Facebook\",\n",
      "  \"relacionamento\": \"criou\"\n",
      "}\n",
      "\n",
      "Texto:\n",
      "\"Jeff Bezos fundou a Amazon em 1994 e transformou-a numa das maiores empresas de comércio eletrônico do mundo.\"\n",
      "Análise de Relacionamento:\n",
      "{\n",
      "  \"entidade1\": \"Jeff Bezos\",\n",
      "  \"entidade2\": \"Amazon\",\n",
      "  \"relacionamento\": \"fundou\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Elon Musk fundou a SpaceX em 2002 com o objetivo de reduzir os custos de transporte espacial e possibilitar a colonização de Marte.\n",
      "\n",
      "Analise a frase e extraia o relacionamento entre duas entidades e, em seguida, produza os resultados indicando: entities e relatiohships entre estas no formato JSON\n",
      "\n",
      "A:\n",
      "{\n",
      "  \"entidade1\": \"Elon Musk\",\n",
      "  \"entidade2\": \"SpaceX\",\n",
      "  \"relacionamento\": \"fundou\",\n",
      "  \"objetivo\": \"reduzir os custos de transporte espacial e possibilitar a colonização de Marte\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "################# Extração de Relações #################\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"Mark Zuckerberg criou o Facebook enquanto estudava em Harvard.\"\n",
    "Análise de Relacionamento:\n",
    "{\n",
    "  \"entidade1\": \"Mark Zuckerberg\",\n",
    "  \"entidade2\": \"Facebook\",\n",
    "  \"relacionamento\": \"criou\"\n",
    "}\n",
    "\n",
    "Texto:\n",
    "\"Jeff Bezos fundou a Amazon em 1994 e transformou-a numa das maiores empresas de comércio eletrônico do mundo.\"\n",
    "Análise de Relacionamento:\n",
    "{\n",
    "  \"entidade1\": \"Jeff Bezos\",\n",
    "  \"entidade2\": \"Amazon\",\n",
    "  \"relacionamento\": \"fundou\"\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Elon Musk fundou a SpaceX em 2002 com o objetivo de reduzir os custos de transporte espacial e possibilitar a colonização de Marte.\n",
    "\n",
    "Analise a frase e extraia o relacionamento entre duas entidades e, em seguida, produza os resultados indicando: entities e relatiohships entre estas no formato JSON\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
